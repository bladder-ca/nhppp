%%%%%%#%#!TEX root = nhppp.tex
% !Rnw weave = knitr

\documentclass[article,nojss]{jss}
% \documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

\usepackage{orcidlink,thumbpdf,lmodern}
\usepackage{framed}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{bm, bbm}
\usepackage{algorithm, algpseudocode}
\usepackage{rotating}
\usepackage{booktabs}

%% new custom commands
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\der}[2]{\frac{d {#1}} {d{#2}}}


\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}
\newcommand{\BI}{\mathbb I}
\newcommand{\BC}{\mathbb C}		\newcommand{\BH}{\mathbb H}
\newcommand{\BR}{\mathbb R}		\newcommand{\BD}{\mathbb D}
\newcommand{\BN}{\mathbb N}		\newcommand{\BQ}{\mathbb Q}
\newcommand{\BS}{\mathbb S}		\newcommand{\BZ}{\mathbb Z}
\newcommand{\BF}{\mathbb F}			\newcommand{\BT}{\mathbb T}
\newcommand{\CA}{\mathcal A}		\newcommand{\CB}{\mathcal B}
\newcommand{\CC}{\mathcal C}		\newcommand{\calD}{\mathcal D}
\newcommand{\CE}{\mathcal E}		\newcommand{\CF}{\mathcal F}
\newcommand{\CG}{\mathcal G}		\newcommand{\CH}{\mathcal H}
\newcommand{\CI}{\mathcal I}		\newcommand{\CJ}{\mathcal J}
\newcommand{\CK}{\mathcal K}		\newcommand{\CL}{\mathcal L}
\newcommand{\CM}{\mathcal M}		\newcommand{\CN}{\mathcal N}
\newcommand{\CO}{\mathcal O}		\newcommand{\CP}{\mathcal P}
\newcommand{\CQ}{\mathcal Q}		\newcommand{\CR}{\mathcal R}
\newcommand{\CS}{\mathcal S}		\newcommand{\CT}{\mathcal T}
\newcommand{\CU}{\mathcal U}		\newcommand{\CV}{\mathcal V}
\newcommand{\CW}{\mathcal W}		\newcommand{\CX}{\mathcal X}
\newcommand{\CY}{\mathcal Y}		\newcommand{\CZ}{\mathcal Z}




% \SweaveOpts{prefix.string=figures/nhppp, concordance=FALSE}

%\VignetteIndexEntry{a guide to the nhppp package}
%\VignetteKeywords{nhppp}
%\VignetteDepends{nhppp,rstream,lifecycle}
%\VignettePackage{nhppp}


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation (and optionally ORCID link)
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Thomas A. Trikalinos~\orcidlink{0000-0002-3990-1848}\\Brown University
   \And Yuliia Sereda~\orcidlink{https://orcid.org/0000-0002-4017-4561}\\Brown University}
\Plainauthor{Thomas A. Trikalinos, Yuliia Sereda}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{\pkg{nhppp}: Simulating Nonhomogeneous Poisson Point Processes in \proglang{R}}
\Plaintitle{nhppp: Simulating Nonhomogeneous Poisson Point Processes in R}
\Shorttitle{Simulating NHPPPs in R}

\Abstract{
 We introduce the \pkg{nhppp} package for simulating events from one dimensional nonhomogeneous Poisson point processes (NHPPPs) in \proglang{R}. Its functions are based on three algorithms that provably sample from a target NHPPP: the time-transformation of a homogeneous Poisson process (of intensity one) via the inverse of the integrated intensity function; the generation of a Poisson number of order statistics from a fixed density function; and the thinning of a majorizing NHPPP via an acceptance-rejection scheme. We present a study of numerical accuracy and time performance of the algorithms and advice on which algorithm to prefer in each situation. Functions available in the package are illustrated with simple reproducible examples.
}

\Keywords{stochastic point processes, counting processes, discrete event simulation, time-to-event simulation, \proglang{R}}
\Plainkeywords{stochastic point processes, counting processes, discrete event simulation, time-to-event simulation, R}

\Address{
  TA Trikalinos\\
  Department of Health Services, Policy \& Practice\\
  \emph{and}\\
  Department of Biostatistics\\
  School of Public Health\\
  Brown University\\
  RI 02912, USA\\
  E-mail: \email{thomas\_trikalinos@brown.edu}
}

\begin{document}

<<setup-block, echo=FALSE, message=FALSE, results='hide'>>=
suppressPackageStartupMessages({
  library("ggplot2")
})
library("bench")
library("latex2exp")
library("nhppp")

options(
  prompt = "R> ", continue = "+  ", width = 70,
  useFancyQuotes = FALSE, out.extra = "",
  cache = TRUE
)
# knitr::opts_chunk$set(warning = FALSE, message = FALSE)
set.seed(2024)

# helper functions and options for the live report generation
full_results <- FALSE
n_simulations <- 10^4
n_bins_wasserstein <- NULL
n_samples_wasserstein <- 1000
if (!full_results) {
  n_simulations <- 100
  n_bins_wasserstein <- 5
  n_samples_wasserstein <- 500
}


sanitize_lambdas <- function(x) {
  x <- gsub(pattern = "l_\\*", replacement = "$\\\\lambda_*$", x = x)
  x <- gsub(pattern = "l_max*", replacement = "$\\\\lambda_{*a}$", x = x)
  x <- gsub(pattern = "l_m2", replacement = "$\\\\lambda_{*c}$", x = x)
  x <- gsub(pattern = "l_m", replacement = "$\\\\lambda_{*b}$", x = x)
  x <- gsub(pattern = "Li_", replacement = "$\\\\Lambda^{-1}$", x = x)
  x <- gsub(pattern = "L_", replacement = "$\\\\Lambda$", x = x)
  return(x)
}


sanitize_strings_for_figures <- function(x) {
  sanitize_lambdas(x)
}


sanitize_strings_for_tables <- function(x) {
  pkglist <- c(
    "nhppp", "reda", "simEd", "NHPoisson", "poisson",
    "IndTestPP", "PtProcess", "spatstat.random"
  )
  for (pkg in pkglist) {
    x <- gsub(pattern = pkg, replacement = paste0("\\\\pkg{", pkg, "}"), x = x)
  }
  x <- sanitize_lambdas(x)
  return(x)
}

pprint <- function(x, digits = 2, use_lt = FALSE) {
  i <- 0
  for (x1 in x) {
    i <- i + 1
    if (use_lt && x1 < 0.5 * 10^(-digits)) {
      x1 <- paste0("<0.", paste0(rep("0", digits - 1), collapse = ""), "1")
    } else {
      x1 <- format(round(x1, digits = digits), nsmall = digits)
    }
    x[i] <- x1
  }
  return(x)
}
@

\section{Introduction} \label{sec:intro}

It is often desirable to simulate series of events (stochastic point processes) so that the intensity of their occurrence varies over time. Examples include events such as the occurrence of death or occurrences of symptoms, infections, or tumors over a person's lifetime. The nonhomogeneous Poisson point process (NHPPP), which generalizes the simpler homogeneous-Poisson, Weibull, and Gompertz point processes, is a widely used model for such series of events. NHPPPs can model complicated event patterns given a suitable intensity function. They are therefore a very useful tool in statistical and mathematical model simulation.

A NHPPP has the properties that the number of events in all non-overlapping time intervals are independent random variables and that, within each time interval, the number of events is Poisson distributed. Thus an NHPPP is a memoryless point process. A large number of phenomena may reasonably conform with these properties.%
\footnote{Other phenomena are not well described by a memoryless point process. An example is new infection events during an epidemic wave, where the number of new infections over time depends on the number of infections in the previous time interval.  Thus, at least in its early phase, an epidemic wave may be better described by, e.g., self-exciting point processes that transiently increase the intensity function after a first event.}

The \pkg{nhppp} package in \proglang{R} contains functions for the simulation of NHPPPs over a one-dimensional carrier space, which we will take to represent time.

We review NHPPPs in Section~\ref{sec:review} and algorithms for sampling from constant rate Poisson point processes in Section~\ref{sec:sample-ppp}. We introduce the three sampling algorithms that are implemented in the package in Section~\ref{sec:general-sampling}. We discuss special functional forms for the intensity function (constant, piecewise constant, linear, and log-linear) in Section~\ref{sec:special_cases}. We describe \pkg{nhppp} versus other \proglang{R} packages that can simulate from one dimensional NHPPPs in Section~\ref{sec:other-R-packages} and present a numerical study in Section~\ref{sec:illustrations}. We summarize in Section~\ref{sec:summary}.


\section{The Poisson point process} \label{sec:review}
\subsection{Definition}
The Poisson point process is a stochastic series of events on the real line. For some sequence of events, let $N(t, t + \Delta t)$ be the number of events in the interval $(t, t  + \Delta t]$. If for some positive intensity $\lambda$ and, as ${\Delta t \rightarrow 0}$,
\begin{equation}\label{eq:definition}
    \begin{aligned}
    \Pr[N(t, t + \Delta t) = 0] =&\  1 - \lambda \Delta t +  o(\Delta t), \\
    \Pr[N(t, t + \Delta t) = 1] =&\  \lambda \Delta t +  o(\Delta t), \\
    \Pr[N(t, t + \Delta t) >1] =&\  o(\Delta t),\text{ and } \\
    N(t, t + \Delta t) \indep&\ N(0, t),
    \end{aligned}
\end{equation}
then that sequence of events is a Poisson point process. In Equation~\eqref{eq:definition}, the third statement demands that events occur one at a time. The fourth statement implies that the process is memoryless: For any time $t_0$, the behavior of the process is independent to what happened before that time.

\subsection{Homogeneous Poisson point process and counting process}\label{sec:ppp-intro}
Assume that the next event after time $t_0$ happens at time $t_0 + X$. It follows from the above definition (see~\citet[par. 4.1]{cox1965theory}) that, for a constant $\lambda$, $X$ is exponentially distributed
\begin{equation}\label{eq:X_PPP}
X \sim \text{Exponential}(\lambda),
\end{equation}
and that, over the compact interval $(a, b]$ the number of events is Poisson distributed
\begin{equation}\label{eq:N_PPP}
N(a, b) \sim \text{Poisson}(\lambda (b-a)).
\end{equation}

Equation~\eqref{eq:X_PPP} generates the homogeneous Poisson point process ${Z_1 = t_0 + X_1, Z_2 = Z_1 + X_2, \dots}$, where $Z_i$ is the time of arrival of event $i$ and the inter-arrival times $X_i$ follow~\eqref{eq:X_PPP}. We will use $Z_{(i)}$ to denote the event $i$ when event are ordered in increasing time.
%
Equation \eqref{eq:N_PPP} describes the corresponding (dual) counting process
${N_1 = N(t_0, Z_1)}, {N_2 = N(t_0, Z_2), \dots}$, where $N_i$ is the total number of events from time $t_0$ to time $Z_i$. The point process (the sequence $[Z_i]$ of event times) and the counting process (the sequence $[N_i]$ of cumulants) are two sides of the same coin.

Sampling from the constant rate point process in~\eqref{eq:X_PPP} is discussed in Section~\ref{sec:sample-ppp}.


\subsection{Non homogeneous Poisson point process and counting process}\label{sec:nhppp-intro}
When the intensity function changes over time, the homogeneous Poisson point process generalizes to its nonstationary counterpart, an NHPPP, with intensity function $\lambda(t) > 0$. For details see \citet[par 4.2]{cox1965theory}. Then the number of events over the interval $(a, b]$ becomes
\begin{equation}\label{eq:N_NHPPP}
N(a, b) \sim \text{Poisson}(\Lambda(a, b)),
\end{equation}
where $\Lambda(a, b) = \int_a^b \lambda(t) \ dt$ is the integrated intensity or cumulative intensity of the NHPPP. Equation~\eqref{eq:N_NHPPP} describes the counting process of the NHPPP, which in turn implies a stochastic point process -- a distribution of events over time.

Here the simulation task is to sample event times from the point process that corresponds to intensity function $\lambda(t)$, or equivalently, to the integrated intensity function $\Lambda(t) = \int_0^t \lambda(s) \ ds$ (Section~\ref{sec:general-sampling}).%
\footnote{With some abuse of notation, we define $\Lambda(t) \coloneqq \Lambda(0, t)$ when  $a=0$.}

\subsubsection{A note on zero intensity processes}
In~\eqref{eq:definition} $\lambda$ is strictly positive but in~\pkg{nhppp} we allow it to be nonnegative. If $\lambda = 0$, $\\{\Pr[N(t, t + \Delta t) = 0] = 1}$ and ${\Pr[N(t, t + \Delta t) \ge 1] = 0}$. This means that no events occur and the stochastic point process in the interval $(t, t + \Delta t]$ is denegerate. Allowing $\lambda(t) \ge 0$ has no bearing on the results of simulations. If
\begin{equation*}
    \lambda(t)  \begin{cases}
    >0, \text{ for } t \in (a, b] \\
    =0, \text{ for } t \in (b, c] \\
    >0, \text{ for } t \in (c, d]
    \end{cases}
\end{equation*}
we can always ignore the middle interval in which no events happen.

\subsection{Properties that are important for simulation}\label{sec:properties}
\subsubsection{Composability and decomposability of NHPPPs}
The definition~\eqref{eq:definition} implies that NHPPPs are composable~\citep[par. 4.2]{cox1965theory}: merging two NHPPPs with intensity functions $\lambda_1(t), \lambda_2(t)$ yields a new NHPPP with intensity function $\lambda(t) = \lambda_1(t) + \lambda_2(t)$. The reciprocal is also true: one can decompose an NHPPP with intensity function $\lambda(t)$ into two NHPPPs, one with intensity function $\lambda_1(t) < \lambda(t)$ and one with intensity function ${\lambda_2(t) = \lambda(t)-\lambda_1(t)}$.%
\footnote{The proof is omitted, but it involves showing that the composition and decomposition yields processes that conform with definition~\eqref{eq:definition}.} An induction argument extends the above to merging and decomposing three or more processes.

The composability and decomposability properties are important for simulation because they
\begin{itemize}
    \item give the flexibility to simulate several parallel NHPPPs independently versus to merge them, simulate from the merged process, and then attribute the realized events to the component processes by assigning the $i$-th event to the $j$-th process with probability $\lambda_j(Z_i) / \lambda(Z_i)$, where $\lambda(t) = \sum \lambda_j(t)$.
    \item motivate a general sampling algorithm (Algorithm~\ref{alg:NHPPP_thinning}, ``thinning''~\citep{lewis1979thinning}) that simulates a target NHPPP with intensity $\lambda_1(t)$ by first drawing events from an easy-to-sample NHPPP with intensity $\lambda(t) >\lambda_1(t)$, and then accepts sample $i$ with probability $\lambda_1(Z_i)/\lambda(Z_i)$.
\end{itemize}

\subsubsection{Transformations of the time axis}
Strictly monotonic transformations of the carrier space of an NHPPP yield an NHPPP~\citep[]{Cinlar1975inversion}. Consider an NHPPP with intensity functions $\lambda(t)$ and a strictly monotonic transformation of the time axis $u: t \mapsto \tau$ that is differentiable once almost everywhere. On the transformed time axis the point process is an NHPPP with intensity function
\begin{equation}\label{eq:transform}
    \rho(\tau) = \lambda(\tau) \left ( \der{u}{t} \right )^{-1}.
\end{equation}

This property is important for simulation because
\begin{itemize}
    \item it motivates the use of another general sampling algorithm (Algorithm~\ref{alg:NHPPP_inversion}, ``time transformation'' or ``inversion'',~\citet{Cinlar1975inversion}): A smart choice for $u$ yields an easy to sample point process. The event times in the original time scale can be obtained as $Z_i = u^{-1}(\zeta_i)$, where $\zeta_i$ is the $i$-th event in the transformed time axis and $u^{-1}$ is the inverse function of $u$.
    \item given that at least $i$ events have realized in the time interval $(a, b]$, it makes it possible to draw events ${Z_{(j)}, j<i}$ given event $Z_{(i)}$. This is useful for simulating earlier events conditional on the occurrence of a subsequent event. Choosing $u(t) := Z_{(i)} - t$ makes the time count backwards from $Z_{(i)}$. In this reversed clock we draw as if in forward time exactly $i-1$ events $\zeta_{(1)}, \zeta_{(2)}, \dots, \zeta_{(i-1)}$. Back transforming yields all preceding events.
\end{itemize}

Table~\ref{tab:simtasks} summarizes the common simulation tasks, such as simulating single events (at most one, exactly one), a series of events (possibly demanding the occurrence of at least one event), or the occurrence of a prior (event $i-1$ given $Z_{(i)}$). The \pkg{nhppp} package implements functions to simulate these tasks for general $\lambda(t)$ or $\Lambda(t)$.

\input{table/tab_simulation_tasks}

\section{Sampling the constant rate Poisson process}\label{sec:sample-ppp}

Sampling the constant rate Poisson process is straightforward. Algorithms~\ref{alg:PPP_t} and~\ref{alg:PPP_order_stats} are two ways to sample event times in interval $(a, b]$ with constant intensity $\lambda$. Algorithm~\ref{alg:PPP_conditional} describes sampling event times conditional on observing at least $k$ events within the interval of interest.


\subsection{Sequential sampling}\label{sec:PPP_t}

\input{algorithms/alg_ppp_sequential}

Algorithm~\ref{alg:PPP_t} samples events sequentially, using the fact that the inter-event times $X_i$ are exponentially distributed with mean $\lambda^{-1}$~\citep[par. 4.1]{cox1965theory}. It involves generation only of exponential random variates, which is cheap on modern hardware. To sample at most $k$ events, change the condition for the while loop in line 3 to
\begin{center}
\textbf{while} {$t <b  \ \& \  |\mathcal{Z}| < k$} \textbf{do}.
\end{center}

The package's \fct{ppp\_sequential} function implements constant-rate sequential sampling that returns a vector with 0 or more event times in the interval $[a, b)$. The \code{range\_t} argument is a two-values vector with the bounds $a, b$. The optional \code{tol} argument is used to get an upper bound on the realized number of events -- using it speeds up the algorithm's implementation in \proglang{R}. Setting the optional argument \code{atmost1} to \code{TRUE} from its default value of \code{FALSE} returns the first event or an empty vector, depending on whether at least one event is drawn in the interval.

<<demo-pppt>>=
library("nhppp")
library("rstream")
ppp_sequential(range_t = c(0, 10), rate = 1, tol = 10^-6, atmost1 = FALSE)
@

All \pkg{nhppp} functions can accept a user provided random number stream object via the \texttt{rng\_stream} option.

<<demo-pppt-rstream>>=
library("rstream")
S <- new("rstream.mrg32k3a")
ppp_sequential(
  range_t = c(0, 10), rate = 1, tol = 10^-6, atmost1 = FALSE,
  rng_stream = S
)
@


\subsection{Sampling using order statistics}\label{sec:PPP_order_stats}
\input{algorithms/alg_ppp_order_stats}

Algorithm~\ref{alg:PPP_order_stats} first draws the number of events in $(a, b]$ from a Poisson distribution. Conditional on the number of events, the event times $Z_i$ are uniformly distributed over $(a, b]$~\citep[par. 4.1]{cox1965theory}. The algorithm returns the order statistics [$Z_{(i)}$], obtained by sorting the event times [$Z_i$] in ascending order. It is necessary to generate all event times to generate the order statistics. Thus, to sample at most $k$ event times we should return the earliest $k$ event times, and line 11 of the Algorithm would be changed to
\begin{center}
\textbf{return} {$\{Z_{(i)} \ | \ i \le k, Z_{(i)} \in \mathcal{Z}\}$}.
\end{center}


The \fct{ppp\_orderstat} function implements constant-rate sampling via the order-statistics algorithm.

<<demo-ppp_orderstat>>=
ppp_orderstat(range_t = c(3.14, 6.28), rate = 1, atmost1 = FALSE)
@

\subsection{Sampling conditional on observing at least $m$ events}\label{sec:PPP_order_stats}

\input{algorithms/alg_ppp_conditional}

Algorithm~\ref{alg:PPP_conditional} is used to generate a point process conditional on observing at least $m$ events. For example, if $\lambda$ is the intensity of tumor generation, it can be used to simulate times of tumor emergence among patients with at least one ($m=1$) tumor. To return the up to $k$ earliest events, we modify line 11 the same way as for Algorithm~\ref{alg:PPP_order_stats}. As an example, in a lifetime simulation we can sample the time of all-cause death by setting in Algorithm~\ref{alg:PPP_conditional} $m=1$, so that at least one event will happen in $(a, b]$, and $k = 1$, to sample only the time of the first event $Z_{(1)}$.

To sample exactly $m$ events, change line 1 of Algorithm~\ref{alg:PPP_conditional} to
\begin{center}
$N \gets m$.
\end{center}

Function \fct{ztppp} simulates times conditional on drawing at least one event - i.e., setting $m=1$ in Algorithm~\ref{alg:PPP_conditional} to sample from a zero truncated Poisson distribution in line 1.
<<demo-ztppp>>=
ztppp(range_t = c(0, 10), rate = 0.001, atmost1 = FALSE)
@

Function \fct{ppp\_n} simulates times conditional on drawing exactly $m$ events.
<<demo-ppp_n>>=
ppp_n(size = 4, range_t = c(0, 10))
@

\section[The general sampling algorithms used in nhppp]{The general sampling algorithms used in \pkg{nhppp}}\label{sec:general-sampling}

The \pkg{nhppp} package uses three well known general sampling algorithms, namely thinning, time transformation or inversion, and order-statistics. These algorithms are efficiently combined to sample from special cases, including cases where the intensity function is a piecewise constant, linear, or log-linear function of time, as described in Section~\ref{sec:sample-nhppp-special}.

The thinning algorithm works with the intensity function $\lambda(t)$, which is commonly available. The inversion and order statistics algorithms have smaller computational cost than the thinning algorithm, but work with the integrated intensity function $\Lambda(t)$ and its inverse $\Lambda^{-1}(z)$, which may not be easily available. The generic function \fct{draw} is a wrapper function that dispatches to specialized functions depending on the provided arguments. It is useful for general use but the specialized functions are usually faster.

<<generic-function-example>>=
l <- function(t) t
L <- function(t) 0.5 * t^2
Li <- function(z) sqrt(2 * z)

draw(
  lambda = l, lambda_maj = l(10), range_t = c(5, 10),
  atmost1 = FALSE, atleast1 = FALSE
) |> head(n = 20)

draw(
  Lambda = L, Lambda_inv = Li, range_t = c(5, 10),
  atmost1 = FALSE, atleast1 = FALSE
) |> head(n = 20)
@

\subsection{The thinning algorithm}\label{sec:thinning}
The thinning algorithm relies on the decomposability of NHPPPs (Section~\ref{sec:properties}) and is described in~\citet{lewis1979thinning}. Let the target NHPPP have intensity function $\lambda(t)$ and $\lambda_*(t) \ge \lambda(t)$ for all $t \in (a, b]$ be a majorizing intensity function. Think of the majorizing function as an easy-to-sample function which is the sum of the intensity of the target point process $\lambda(t)$ and the intensity $\lambda_{reject}(t)$ of its complementary point-process,
$$\lambda_*(t) = \lambda(t) + \lambda_{reject}(t).$$

The acceptance-rejection scheme in Algorithm~\ref{alg:NHPPP_thinning} generates proposal samples with intensity function $\lambda_*(t)$ and stochastically attributes them to the target process (to keep, with probability $\lambda(Z)/\lambda_*(Z)$) or its complement.

\input{algorithms/alg_nhppp_thinning}

To sample the earliest $k$ points, one can exit the for loop in lines 4-9 when $k$ events have been sampled in line 7, or, alternatively, return the first up to $k$ points in line 11.

A measure of the efficiency of Algorithm~\ref{alg:NHPPP_thinning} is the proportion of samples that are accepted, which is
\begin{equation}\label{eq:thinning-efficiency}
\frac{\int_a^b{\lambda(t) \textrm{ d}t}}{\int_a^b{\lambda_*(t) \textrm{ d}t}}
\end{equation}
on average. Thus, the closer $\lambda_*(t)$ is to $\lambda(t)$, the more efficient the algorithm.

In practice, $\lambda_*(t)$ can be chosen as one of the special cases in Section~\ref{sec:special_cases}, for which we have fast sampling algorithms. For example, it can be a piecewise constant majorizer. Algorithm~\ref{alg:lambda_majorizer} in Appendix~\ref{app:piecewise_majorizer} can be automatically generate a piecewise constant majorizer function for the general case of a $K$-Lipschitz continuous intensity function.

The \pkg{nhppp} package has functions that sample from time-varying intensity functions. The first function, \fct{draw\_intensity}, expects a user-provided linear ($\lambda_*(t) = \alpha + \beta t$) or log-linear ($\lambda_*(t) = e^{\alpha + \beta t}$) majorizer function.

<<demo-draw_intensity>>=
lambda_fun <- function(t) exp(0.02 * t)

draw_intensity(
  lambda = lambda_fun, # linear majorizer
  lambda_maj = c(intercept = 1.01, slope = 0.03),
  exp_maj = FALSE, range_t = c(0, 10), atmost1 = FALSE
)

draw_intensity(
  lambda = lambda_fun, # log-linear majorizer
  lambda_maj = c(intercept = 0.01, slope = 0.03),
  exp_maj = TRUE, range_t = c(0, 10), atmost1 = FALSE
)
@


The second function, \fct{draw\_intensity\_step}, expects a user-provided piecewise linear majorizer
\begin{align*}
    \lambda_*(t) = \begin{cases}
    \lambda_1 &\textrm{ for } t \in [a_1, b_1) = [a, b_1), \\
    \dots &\\
    \lambda_m &\textrm{ for } t \in [a_m, b_m) \textrm{ with } a_{m} = b_{m-1}, \\
    \dots &\\
    \lambda_M &\textrm{ for } t \in [a_M, b_M) = [a_M, b),
    \end{cases}
\end{align*}
which is specified as a vector of length $M+1$ including the points $(a, [b_m]_{m=1}^M)$ and a vector of length $M$ with the values $[\lambda_m]_{m=1}^M$ in each subinterval of $(a, b]$. For example, the following code splits the interval $(0, 10]$ into $M=10$ subintervals of length one. Because \fct{lambda\_fun} is strictly increasing, its value at the upper bound of each subinterval is the supremum of the interval (Appendix~\ref{app:piecewise_majorizer}, Algorithm~\ref{alg:lambda_majorizer}).


<<demo-draw_intensity_step>>=
draw_intensity_step(
  lambda = lambda_fun,
  lambda_maj_vector = lambda_fun(1:10), # 1:10 (10 intensity values)
  times_vector = 0:10, # 0:10 (11 interval bounds)
  atmost1 = FALSE
)
@

\subsection{The time transformation or inversion algorithm}\label{sec:inversion}
Algorithm~\ref{alg:NHPPP_inversion} implements the time transformation or inversion algorithm from~\citet{Cinlar1975inversion} and also~\citet[par. 4.2]{cox1965theory}. As mentioned in Section~\ref{sec:properties}, strictly monotonic transformations of the carrier space (here, time) of a Poisson Point Process yield another Poisson Point Process. In equation~\eqref{eq:transform}, choosing the transformation $\tau = u(t) = \Lambda(t)$, so that $\der{u(t)}{t} = \lambda(t)$, results in $\rho(\tau) = 1$.

This means (proof sketched in~\citet[par. 4.2]{cox1965theory}) that we can sample points from a Poisson point process with intensity one over the interval $(\tau_a, \tau_b] = (\Lambda(a), \Lambda(b)]$. Via a similar argument, we transform event times sampled on the transformed scale back to the original scale using $g(t)=\Lambda^{-1}(\tau)$. The transformations $u(\cdot), g(\cdot)$ are not unique -- at least up to the group of affine transformations.

Function \fct{draw\_cumulative\_intensity\_inversion} works with a cumulative intensity function $\Lambda(t)$ and its inverse $\Lambda^{-1}(z)$, if available. If the inverse function is not available (argument \texttt{Lambda\_inv = NULL}), the Brent bisection algorithm is used to invert $\Lambda(t)$ numerically, at a performance cost~\citep{brent-bisection}.

<<demo-draw_cumulative_intensity_inversion>>=
Lambda_fun <- function(t) 50 * exp(0.02 * t) - 50
Lambda_inv_fun <- function(z) 50 * log((z + 50) / 50)

draw_cumulative_intensity_inversion(
  Lambda = Lambda_fun,
  Lambda_inv = Lambda_inv_fun,
  range_t = c(5, 10.5),
  range_L = Lambda_fun(c(5, 10.5)),
  atmost1 = FALSE
)
@

\input{algorithms/alg_nhppp_inversion}

\subsection{The order statistics algorithm}\label{sec:order-stats}
The general order statistics algorithm (Algorithm~\ref{alg:NHPPP_order_stats}) is a direct generalization of Algorithm~\ref{alg:PPP_order_stats}. It first draws the number $N$ of realized events. Conditional on $N$
\begin{equation}\label{eq:nhppp_orderstats1}
\begin{aligned}
U_{(i)} &= \frac{\Lambda(Z_{(i)}) - \Lambda(a)}{\Lambda(b)- \Lambda(a)} \sim \textrm{Uniform}(0,1), \\
Z_{(i)} &= \Lambda^{-1} \Big ( \Lambda(a) + U_{(i)} \big( \Lambda(b)- \Lambda(a) \big) \Big),
\end{aligned}
\end{equation}
as discussed in~\citet{lewis1979thinning}. Algorithm~\ref{alg:NHPPP_order_stats} makes the above explicit.

\input{algorithms/alg_nhppp_order_stats}

Sampling up to $k$ earliest points means returning the up to $k$ earliest event times. If $\Lambda(t)$ is a positive linear function of time, $\lambda$ is constant and Algorithm~\ref{alg:NHPPP_order_stats} becomes Algorithm~\ref{alg:PPP_order_stats}.

To sample conditional on observing at least $m$ events in the interval $(a, b]$, modify line 1 of Algorithm~\ref{alg:NHPPP_order_stats} to sample from a $(m-1)$-truncated Poisson distribution (Appendix~\ref{app:conditional_sampling}, Algorithm~\ref{alg:NHPPP_conditional}).
\begin{center}
$N \gets N \sim \mathrm{TruncatedPoisson}_{N \ge m}\big(\Lambda(b) - \Lambda(a)\big)$.
\end{center}


Function \fct{draw\_cumulative\_intensity\_orderstats} works with a cumulative intensity function $\Lambda(t)$ and its inverse $\Lambda^{-1}(z)$, if available.
Function \fct{ztdraw\_cumulative\_intensity} conditions that at least one event is sampled in the interval. If the inverse function is not available (argument \texttt{Lambda\_inv = NULL}), the Brent bisection algorithm is used to invert $\Lambda(t)$ numerically, at a performance cost.

<<demo-draw_cumulative_intensity_orderstats>>=
draw_cumulative_intensity_orderstats(
  Lambda = Lambda_fun,
  Lambda_inv = Lambda_inv_fun,
  range_t = c(4.1, 7.6),
  atmost1 = FALSE
)
@

<<demo-ztdraw_cumulative_intensity>>=
ztdraw_cumulative_intensity(
  Lambda = Lambda_fun,
  Lambda_inv = Lambda_inv_fun,
  range_t = c(4.1, 7.6),
  atmost1 = FALSE
)
@





\section{Special cases}\label{sec:special_cases}

The \pkg{nhppp} package implements several special cases where the intensity function $\lambda(\cdot)$, the integrated intensity function $\Lambda(\cdot)$, and its inverse $\Lambda^{-1}(\cdot)$ have straightforward analytical expressions.

\subsection{Sampling a piecewise constant NHPPP}\label{sec:sample-nhppp-pc}

Functions \fct{draw\_sc\_step} and \fct{draw\_sc\_step\_regular} sample piecewise constant intensity functions based on Algorithm~\ref{alg:NHPPP_inversion}. The first can work with unequal-length subintervals $(a_m, b_m]$. The second results in a small computational time improvement when all subintervals are of equal length.
<<demo-draw_sc_step>>=
draw_sc_step(
  lambda_vector = 1:5, times_vector = c(0.5, 1, 2.4, 3.1, 4.9, 5.9),
  atmost1 = FALSE, atleast1 = FALSE
)

draw_sc_step_regular(
  lambda_vector = 1:5, range_t = c(0.5, 5.9), atmost1 = FALSE,
  atleast1 = FALSE
)
@


Function \fct{vdraw\_sc\_step\_regular} is a vectorized version of \fct{draw\_sc\_step\_regular}. It returns a matrix with one event series per row, and as many columns as the maximum number of events across all draws.

<<demo-vdraw_sc_step_regular>>=
vdraw_sc_step_regular(
  lambda_matrix = matrix(runif(20), ncol = 5), range_t = c(1, 4),
  atmost1 = FALSE
)
@

\subsection{Sampling NHPPPs with linear and log-linear intensities}\label{sec:sample-nhppp-special}

Functions \fct{draw\_sc\_linear} and \fct{ztdraw\_sc\_linear} sample zero or more and at least one event, respectively, from NHPPPs with linear intensity functions. An optional argument (\texttt{atmost1}) returns the first event only.
\begin{align*}
    \lambda(t) =
    \begin{cases}
        \alpha + \beta t &\text{ for } t \in [a, b), t>-\frac{\alpha}{\beta} \\
        0 &\textrm{ otherwise}
    \end{cases}.
\end{align*}

<<demo-zt-draw_sc_linear>>=
draw_sc_linear(
  alpha = 3, beta = -0.5, range_t = c(0, 10),
  atmost1 = FALSE
)
ztdraw_sc_linear(
  alpha = 0.5, beta = 0.2, range_t = c(0, 10),
  atmost1 = FALSE
)
@

An analogous set of functions (\fct{[nhppp|ztnhppp]\_sc\_loglinear}) samples from log-linear intensity functions

\begin{align*}
    \lambda(t) =
    \begin{cases}
        e^{\alpha + \beta t} &\text{ for } t \in [a, b) \\
        0 &\textrm{ otherwise}
    \end{cases}.
\end{align*}
The sampling algorithm is a variation of Algorithm~\ref{alg:NHPPP_inversion}, as described in~\citet{lewis1976linear}. Example usage follows.
<<demo-draw_sc_loglinear>>=
draw_sc_loglinear(
  alpha = 1, beta = -0.02, range_t = c(8, 10),
  atmost1 = FALSE
)
@

<<demo-ztdraw_sc_loglinear>>=
ztdraw_sc_loglinear(
  alpha = 1, beta = -0.02, range_t = c(9.8, 10),
  atmost1 = FALSE
)
@


\section[Comparisons with other R packages]{Comparisons with other \proglang{R} packages}\label{sec:other-R-packages}

Table~\ref{tab:R-packages} lists five \proglang{R} packages that simulate from NHPPPs, including \pkg{nhppp}.

Package \pkg{reda}~\citep{reda-package} focuses on recurrent event data analysis and can simulate NHPPPs with the inversion and the thinning algorithms using the \fct{simEvent} function. It can take function object arguments for $\lambda(t)$. When using the thinning algorithm, it takes a constant majorizer. For the inversion algorithm, it approximates $\Lambda(t)$ and its inverse numerically, at a computational cost.

Package \pkg{simEd}~\citep{simEd-package} includes various functions for simulation education. Function \fct{thinning} implements the homonymous algorithm for drawing points from an NHPPP. Users can specify the intensity function and a piecewise constant or linear majorizer function.

Package \pkg{IndTestPP}~\citep{IndTestPP-package} provides a framework for exploring the dependence between two or more realizations of point processes. It includes the ancillary function~\fct{simNHPc} for simulating NHPPPs with the inversion or thinning algorithms. The function's argument is a piecewise constant approximation of the intensity function via a vector of evaluations, each corresponding to unit length subintervals. This resolution may not be adequate to simulate processes that change fast over a unit time interval.

Package \pkg{NHPoisson}~\citep{NHPoisson-jss, NHPoisson-package} fits NHPPP models to data and is not really geared towards mathematical simulation. Its \fct{simNHP.fun} function provides the ability for simulation-based inference via an implementation of the inversion algorithm. This function is designed to work with the package's inference machinery and is not practical to use for simulation. This is because the user has no direct control over the function's rescaling of the time axis.

The claimed advantage of \pkg{nhppp} over the existing packages is that
\begin{itemize}
\item it samples from the target NHPPP and not from a numerical approximation thereof, e.g., as \pkg{IndTestPP} does.
\item It can  sample conditional on observing at least one event in the interval, which no other package implement.
\item It accepts user-provided random number stream objects, which is useful for implementing simulation variance reduction techniques such as common random numbers~\citep{wright1979crn} and antithetic variates~\citep{hammersley1956av}.
\item It is fast, even though it is currently implemented primarily in \proglang{R}-only code. It has specialized functions to leverage additional information about the process, such as $\Lambda(t), \Lambda^{-1}(z)$, when available, which can result in faster simulation  use the cumulative intensity function and its inverse, often at a computational speed advantage. It also includes a vectorized function, \fct{vdraw\_sc\_step\_regular}, which draws from piecewise constant intensity functions.%
\footnote{More functions will be vectorized in future releases.}
\end{itemize}

\input{table/tab_other_R_packages}

\newpage
\section{Illustrations} \label{sec:illustrations}

Depending on the application, we may have access to the intensity function
or the integrated intensity function.
We compared the \proglang{R} packages in Table~\ref{tab:R-packages} for sampling from a non-monotone and highly nonlinear intensity function for which the integrated intensity function is known analytically.

\subsection{The target NHPPP to be simulated}\label{sec:illustration-target}

Consider the example
\begin{equation}\label{eq:illustration}
\begin{aligned}
\lambda(t) &= e^{rt}(1+\sin wt), \\
\Lambda(t) &= \dfrac{e^{rt}(r\sin wt - w\cos wt) + w}{r^2+w^2}+\dfrac{e^{rt}-1}{r} %- \dfrac{r^2-r+1}{r^3+r}
\end{aligned}
\end{equation}
of a sinusoidal intensity function $\lambda(t)$ scaled to have an exponential amplitude and one of its antiderivatives $\Lambda(t)$, with such a constant term that $\Lambda(0)=0$.  For the numerical study we set $r=0.2$, $w=1$, and $t \in (0, 6\pi]$. There is no  analytic inverse function for this example.
However, we can precompute \fct{Li}, a good numerical approximation to $\Lambda^{-1}(z)$. We will use it in Section~\ref{sec:example-time-performance} to compare the time performance of functions that use the inversion and order statistics algorithms when $\Lambda^{-1}$ is available versus not.
<<example-functions>>=
l <- function(t) (1 + sin(t)) * exp(0.2 * t)
L <- function(t) {
  exp(0.2 * t) * (0.2 * sin(t) - cos(t)) / 1.04 +
    exp(0.2 * t) / 0.2 - 4.038462
}
Li <- approxfun(
  x = L(seq(0, 6 * pi, 10^-3)),
  y = seq(0, 6 * pi, 10^-3), rule = 2
)
@

<<example-K-Lipschitz-value, echo=FALSE, include=FALSE>>=
# to get a bound for K
abs_l_prime_fun <- function(t, w = 1) abs((0.2 + 0.2 * sin(w * t) + w * cos(w * t))) * exp(0.2 * t)
K_lipschitz <- abs_l_prime_fun(6 * pi)
@

<<example-majorizers, echo=FALSE, include=FALSE>>=
M <- 20
t_breaks <- seq(0, 6 * pi, length.out = M + 1)
l_m <- get_step_majorizer(
  fun = l, breaks = t_breaks,
  is_monotone = FALSE, K = K_lipschitz
)
l_star <- stats::approxfun(
  x = t_breaks[1:M], y = l_m, method = "constant",
  rule = 2
)

l_m2 <- c()
for (m in 1:M) {
  t_tmp <- seq(t_breaks[m], t_breaks[m + 1], length.out = 10000)
  y_tmp <- l(t_tmp)
  l_m2 <- c(l_m2, y_tmp[which(y_tmp == max(y_tmp))] + K_lipschitz * (t_tmp[2] - t_tmp[1]) / 2)
}
l_star2 <- stats::approxfun(
  x = t_breaks[1:M], y = l_m2, method = "constant",
  rule = 2
)
@


<<example-function-plot, fig.cap="The $\\lambda(t)$ (left) and $\\Lambda(t)$ used in the illustration. Also shown three majorizing functions (left panel, marked a, b, c) that are used with the thinning algorithm in the analyses.", fig.height=4, fig.width=8, echo=FALSE>>=
plot_base <- ggplot(data = data.frame(x = seq(0, 6 * pi, length.out = 200)), aes(x)) +
  xlab("Time") +
  theme_bw() +
  theme(text = element_text(size = 18))
plot_l <- plot_base +
  geom_function(fun = l, xlim = c(0, 6 * pi), color = "red") +
  geom_function(
    fun = function(x) l(6 * pi),
    xlim = c(0, 6 * pi), n = 200, color = "blue", linetype = "longdash"
  ) +
  geom_step(
    data = data.frame(x = t_breaks, l_m = l_m[c(1:M, M)]), aes(y = l_m),
    color = "black"
  ) +
  geom_step(
    data = data.frame(x = t_breaks, l_m2 = l_m2[c(1:M, M)]), aes(y = l_m2),
    color = "black", linewidth = 1
  ) +
  geom_text(aes(x = 1, y = l(6 * pi) + 3, label = "a")) +
  geom_text(aes(x = 1, y = l_star(1) + 3, label = "b")) +
  geom_text(aes(x = 1, y = l_star2(1) + 3, label = "c")) +
  ylab("Intensity")
plot_L <- plot_base +
  geom_function(fun = L, xlim = c(0, 6 * pi), color = "red") +
  ylab("Integrated intensity")
gridExtra::grid.arrange(plot_l, plot_L, nrow = 1)
@

Figure~\ref{fig:example-function-plot} graphs the intensity function and three majorizing functions over the interval of interest, which will be needed for the thinning algorithm.

The first, $\lambda_{*a}(t) = \Sexpr{pprint(l(6*pi), 2)}$, shown as a dashed blue line, is is a constant majorizer equal to the maximum of the intensity function. A constant majorizer may be a practical choice when only an upper bound is known for $\lambda(t)$.
From~\eqref{eq:thinning-efficiency}, the efficiency of the thinning algorithm using this majorizer is
$\Sexpr{ pprint(L(6*pi)/(l(6*pi)*6*pi), 3) }$.

The second, $\lambda_{*b}(t)$, shown as a thin black line, is a piecewise constant envelope generated automatically from Algorithm~\ref{alg:lambda_majorizer} (Appendix~\ref{app:piecewise_majorizer}) with $\Sexpr{M}$ equal-length subintervals and ${K = \Sexpr{pprint(K_lipschitz, 2)}}$, where the Lipschitz cone coefficient $K$ is equal to the maximum value of $|\der{\lambda(t)}{t}|$ in the interval, attained at $6\pi$.
The efficiency of the thinning algorithm using this majorizer is
$\Sexpr{pprint(L(6*pi)/sum(l_star(t_breaks[1:M])*(6*pi / M)), 3) }$.

The third, $\lambda_{*c}(t)$, shown as a thicker black line, is a tighter piecewise constant majorizer with the same $\Sexpr{M}$ equal-length subintervals that is constructed by finding a least upper bound in each subinterval. The efficiency of the thinning algorithm with the third
majorizer is
$\Sexpr{pprint(L(6*pi)/sum(l_star2(t_breaks[1:M])*(6*pi / M)), 3) }$.

\newpage
\subsection{Simulation functions and algorithms}\label{sec:methods-sim}

We sampled series of events from the target NHPPP using the packages and functions listed in Table~\ref{tab:R-packages}. We repeated the sampling $\Sexpr{n_simulations}$ times, recording all simulated points (event times). We also recorded the median computation time for drawing one series of events with single-threaded computation on modern hardware.

From the \pkg{nhppp} package we use
\begin{enumerate}
\item  two functions that take as argument the intensity function and are based on Algorithm~\ref{alg:NHPPP_thinning} (thinning): \fct{draw\_intensity}, which uses linear majorizers such as $\lambda_{*a}$, and \fct{draw\_intensity\_step}, which uses piecewise constant majorizers such as $\lambda_{*b}$ and $\lambda_{*c}$ in the example.
\item  Function \fct{draw\_cumulative\_intensity\_inversion}, which takes as argument the cumulative intensity function $\Lambda(t)$ and is based on Algorithm~\ref{alg:NHPPP_inversion} (time transformation/inversion), and
\item  function \fct{draw\_cumulative\_intensity\_orderstats}, which also uses $\Lambda(t)$ and is based on Algorithm~\ref{alg:NHPPP_order_stats} (order statistics).
\end{enumerate}

Regarding the other \proglang{R} packages in Table~\ref{tab:R-packages}, we used all except for \pkg{NHPoisson}, whose simulation function is tailored to supporting simulation based inference for data analysis and is not practical to use as a standalone function.%
\footnote{The implementation does not allow the user to control the scaling of the time axis in a practical way.}
However, its source code/algorithm is very similar to that of the \pkg{IndTestPP} simulation function, which is developed by the same authors.

We used the metrics in Table~\ref{tab:sim_metrics} to assess simulation performance with each function. We compared the empirical versus the simulated distributions of number of events and event times over $J = 100$ simulation runs.

\subsection{Simulation performance with respect to number of events}\label{sec:sim-counts}

We calculated the absolute and relative bias in the first two moments of the empirical distribution in the counts of events, the bounds of equal-tailed confidence intervals at the 95, 90, 75, and 50 percent levels, a $\chi^2$-distributed goodness of fit statistic and its $p$-value, and the Wasserstein-1 distance $W_1$ between the empirical and the theoretical count distributions and the asymptotic one sided $p$~value to reject whether $W_1 = 0$ according to~\citet{sommerfeld2018inference}. $W_1$ is the smallest mass that has to be redistributed so that one distribution matches the other. $W_1$ is equal to the unsigned area between the cumulative distribution functions of the compared distributions. For example, $W_1 = 5.25$ means that the mass that must be moved to transform one density to the other is no less than $5.25$ counts and a $W_1 = 0$ implies perfect fit.

\input{table/tab_simulation_metrics}

<<define-functions-for-counts,echo=FALSE, include=FALSE, cache=TRUE>>=
get_chi2_counts <- function(x, mu) {
  # O, E counts
  Q <- seq(0.0001, 0.9999, length.out = 100)
  E <- qpois(Q, mu)
  O <- quantile(x, Q)
  # Goodness of Fit p value
  GOF <- sum(((O - E)^2) / E)
  pval <- pchisq(GOF, length(O) - 1, lower.tail = F)
  return(list(chi2 = GOF, pval = pval))
}
get_CIs_counts <- function(x, mu, CIs = c(95, 90, 75, 50)) {
  pct <- c()
  for (CI in CIs) {
    pr <- 1 - CI / 100
    pct <- c(pct, pr / 2, 1 - pr / 2)
  }
  theoretical <- matrix(stats::qpois(pct, mu), byrow = TRUE, ncol = 2)
  empirical <- matrix(as.integer(quantile(x, pct)), byrow = TRUE, ncol = 2)
  return(list(theoretical = theoretical, empirical = empirical))
}
get_wasserstein_counts <- function(x, mu, tmp_seed = 123, B = 1000, L = NULL) {
  # theoretical distribution XL, XU counts
  XL <- stats::qpois(p = 0.00001, lambda = mu)
  XU <- stats::qpois(p = 0.99999, lambda = mu)

  # empirical distribution PMF
  tabx <- table(x)
  the_ints <- as.integer(names(tabx))
  xl <- min(the_ints)
  xu <- max(the_ints)
  empirical_pmf <- c()
  for (xi in min(xl, XL):max(xu, XU)) {
    tmp <- tabx[which(the_ints == xi)]
    if (length(tmp) == 0) tmp <- 0
    empirical_pmf <- c(empirical_pmf, tmp)
  }
  empirical_pmf <- matrix(empirical_pmf / sum(empirical_pmf), ncol = 1)
  theoretical_pmf <- matrix(stats::dpois(x = min(xl, XL):max(XU, xu), lambda = mu), ncol = 1)
  theoretical_pmf <- theoretical_pmf / sum(theoretical_pmf)

  cost_mat <- as.matrix(stats::dist(arrayInd(1:nrow(theoretical_pmf), .dim = nrow(theoretical_pmf))))
  w1 <- otinference::wassDist(a = empirical_pmf[, 1], b = theoretical_pmf[, 1], distMat = cost_mat, p = 1)

  L <- if (!is.null(L)) as.integer(L) else nrow(theoretical_pmf)
  current_seed <- .Random.seed
  set.seed(tmp_seed)
  pval <- otinference::binWDTest(x = empirical_pmf, y = theoretical_pmf, L = L, B = B)
  set.seed(current_seed)
  return(list(w1 = w1, pval = pval))
}

performance_metrics_counts <- function(x, mu, digits = 3, tmp_seed = 123, B = 500, L = 20) {
  mean_x <- mean(x)
  B_mu <- mean_x - mu
  var_x <- var(x)
  B_V <- var_x - mu
  gof <- get_chi2_counts(x = x, mu = mu)
  cis <- get_CIs_counts(x, mu, CIs = c(95, 90, 75, 50))
  WD <- get_wasserstein_counts(x = x, mu = mu, tmp_seed = tmp_seed, B = B, L = L)

  values <- c(
    `Sample mean` = pprint(mean_x, digits),
    `$B_\\mu$` = pprint(B_mu, digits),
    `$B_{\\mu, rel}$` = pprint(100 * B_mu / mu, digits),
    `Sample variance` = pprint(var_x, digits),
    `$B_V$` = pprint(B_V, digits),
    `$B_{V, rel}$` = pprint(100 * B_V / mu, digits),
    `Goodness of fit, $\\chi^2$ [$p$~value]` = paste0(pprint(gof$chi2, digits), " [", pprint(gof$pval, digits, use_lt = T), "]"),
    `$W_1$ [$p$~value]` = paste0(pprint(WD$w1, digits, use_lt = T), " [", pprint(WD$pval, digits, use_lt = T), "]"),
    `Bounds 95\\% CI` = paste0("[", paste0(cis$empirical[1, ], collapse = ", "), "]"),
    `Bounds 90\\% CI` = paste0("[", paste0(cis$empirical[2, ], collapse = ", "), "]"),
    `Bounds 75\\% CI` = paste0("[", paste0(cis$empirical[3, ], collapse = ", "), "]"),
    `Bounds 50\\% CI` = paste0("[", paste0(cis$empirical[4, ], collapse = ", "), "]")
  )
  names(values)[length(values) - 3] <- paste0("Equal tail 95\\% CI = [", paste0(cis$theoretical[1, ], collapse = ", "), "]")
  names(values)[length(values) - 2] <- paste0("Equal tail 90\\% CI = [", paste0(cis$theoretical[2, ], collapse = ", "), "]")
  names(values)[length(values) - 1] <- paste0("Equal tail 75\\% CI = [", paste0(cis$theoretical[3, ], collapse = ", "), "]")
  names(values)[length(values) - 0] <- paste0("Equal tail 50\\% CI = [", paste0(cis$theoretical[4, ], collapse = ", "), "]")

  return(values)
}
@

The results for the~\pkg{nhppp} functions in Table~\ref{tab:nhppp-results-counts} and Figure~\ref{fig:ecdf-nhppp-pkg-counts} suggest excellent simulation performance.

<<simulate-nhppp-pkg, include=FALSE, echo=FALSE>>=
simulate_nhppp_pkg <- function() {
  Thinning_const <- nhppp::draw_intensity(
    lambda = l,
    lambda_maj = l(6 * pi),
    range_t = c(0, 6 * pi),
    atmost1 = FALSE
  )
  Thinning_star <- nhppp::draw_intensity_step(
    lambda = l,
    lambda_maj_vector = l_star(t_breaks[1:M]),
    times_vector = t_breaks,
    atmost1 = FALSE
  )
  Thinning_star2 <- nhppp::draw_intensity_step(
    lambda = l,
    lambda_maj_vector = l_star2(t_breaks[1:M]),
    times_vector = t_breaks,
    atmost1 = FALSE
  )
  Inversion <- nhppp::draw_cumulative_intensity_inversion(
    Lambda = L,
    Lambda_inv = Li, # numbers same with this NULL
    range_t = c(0, 6 * pi),
    atmost1 = FALSE
  )
  OrderStats <- nhppp::draw_cumulative_intensity_orderstats(
    Lambda = L,
    Lambda_inv = Li, # numbers same with this NULL
    range_t = c(0, 6 * pi),
    atmost1 = FALSE
  )
  return(list(
    "Thinning l_*=l_max" = Thinning_const,
    "Thinning l_*=l_m" = Thinning_star,
    "Thinning l_*=l_m2" = Thinning_star2,
    "Inversion" = Inversion,
    "Order statistics" = OrderStats
  ))
}

nhppp_pkg_event_series <- parallel::mcmapply(
  function(ni) {
    simulate_nhppp_pkg()
  },
  1:n_simulations,
  mc.cores = parallel::detectCores() - 1
)

nhppp_simulated_counts <- apply(nhppp_pkg_event_series, 2, function(x) sapply(x, length))

metrics_counts_nhppp_pkg <- apply(nhppp_simulated_counts, 1, performance_metrics_counts,
  mu = L(6 * pi) - L(0), digits = 3,
  B = n_samples_wasserstein, L = n_bins_wasserstein
)
@


<<performance-nhppp-pkg-counts, results='asis', echo=FALSE>>=
print(
  xtable::xtable(metrics_counts_nhppp_pkg,
    type = "latex",
    caption = "Simulated total number of events with \\pkg{nhppp} functions for the illustration example. Equal tail $p$\\% CI: a confidence interval whose bounds are the $p/2$ and $(1-p/2)$ count percentiles of the respective cumulative distribution function.",
    label = "tab:nhppp-results-counts"
  ),
  math.style.negative = TRUE,
  sanitize.text.function = sanitize_strings_for_tables,
  scalebox = .77
)
@


<<functions-to-plot-ecdfs, echo=FALSE, include=FALSE>>=
plot_ecdf <- function(index, simulated_counts, Lambda = L) {
  x <- simulated_counts[[index]]

  t_vec <- as.integer(seq(stats::qpois(0.0001, Lambda(6 * pi) - Lambda(0)), stats::qpois(0.9999, L(6 * pi) - L(0)), length.out = 1000))
  density_vec <- stats::dpois(t_vec, Lambda(6 * pi) - Lambda(0))
  pct_vec <- stats::ppois(t_vec, Lambda(6 * pi) - Lambda(0))

  p <- ggplot(NULL) +
    geom_step(aes(x = t_vec, y = pct_vec), linetype = "solid", color = "red", linewidth = 0.5) +
    labs(x = "Count", y = "Cumulative density") +
    coord_cartesian(xlim = c(120, 225)) +
    theme_bw() +
    theme(text = element_text(size = 15)) +
    stat_ecdf(aes(x = simulated_counts[index, ]), linetype = "solid", color = "black") +
    ggtitle(TeX(sanitize_strings_for_figures(rownames(simulated_counts)[index])))
  return(p)
}
@


<<ecdf-nhppp-pkg-counts, fig.cap="Theoretical (red) and empirical (black) cumulative distribution functions for event counts in the illustration example with \\pkg{nhppp} functions. The unsigned area between the theoretical and empirical curves equals the Wasserstein-1 distance in Table~\\ref{tab:nhppp-results-counts}.", fig.height=8, fig.width=12, echo=FALSE>>=
epdf_nhppp_counts <- gridExtra::grid.arrange(
  grobs = lapply(1:nrow(nhppp_simulated_counts),
    plot_ecdf,
    simulated_counts = nhppp_simulated_counts
  ),
  ncol = 3
)
@



<<simulate-R-packages, echo=FALSE, cache=TRUE>>=
simulate_r_pkgs <- function() {
  start <- 0
  stop <- 6 * pi

  reda_thinning_lmax <- reda::simEvent(
    rho = l,
    rhoMax = l(stop),
    origin = start, endTime = stop,
    recurrent = TRUE,
    method = "thinning"
  )

  reda_inversion <- reda::simEvent(
    rho = l,
    origin = start, endTime = stop,
    recurrent = TRUE,
    method = "inversion"
  )

  simEd_thinning_lmax <- simEd::thinning(
    maxTime = ceiling(stop),
    intensityFcn = l,
    majorizingFcn = function(x) l(stop),
    majorizingFcnType = NULL,
    seed = NA,
    maxTrials = Inf,
    plot = FALSE
  )

  t_seq_2 <- seq(start, stop, length.out = ceiling(stop))

  IndTestPP_thinning <- IndTestPP::simNHPc(
    lambda = l(t_seq_2),
    fixed.seed = NULL,
    algor = "Thinning"
  )$posNH

  IndTestPP_inversion <- IndTestPP::simNHPc(
    lambda = l(t_seq_2),
    fixed.seed = NULL,
    algor = "Inversion"
  )$posNH


  return(list(
    "reda thinning (l_*=l_max)" = reda_thinning_lmax,
    "reda inversion" = reda_inversion,
    "simEd thinning (l_*=l_max)" = simEd_thinning_lmax,
    "IndTestPP thinning (no l_*)" = IndTestPP_thinning,
    "IndTestPP inversion" = IndTestPP_inversion
  ))
}


old_seed <- .Random.seed
set.seed(2024)
cl <- parallel::makeCluster(parallel::detectCores() - 1)
start <- 0
stop <- 6 * pi
parallel::clusterExport(cl, c("simulate_r_pkgs", "l"))
r_pkg_event_series <- parallel::parSapply(cl,
  X = 1:n_simulations,
  FUN = function(i) {
    i <- simulate_r_pkgs()
  }
)
parallel::stopCluster(cl)
set.seed(.Random.seed)
r_pkg_times <- apply(r_pkg_event_series, 1, unlist)

r_pkgs_simulated_counts <- apply(t(r_pkg_event_series), 1, function(x) sapply(x, length))
metrics_counts_r_pkgs <- apply(r_pkgs_simulated_counts, 1, performance_metrics_counts,
  mu = L(6 * pi) - L(0), digits = 3,
  B = n_samples_wasserstein, L = n_bins_wasserstein
)
colnames(metrics_counts_r_pkgs) <- c(
  "reda thinning, l_*=l_max",
  "reda inversion",
  "simEd thinning, l_*=l_max",
  "IndTestPP thinning, no l_*",
  "IndTestPP inversion"
)
@

The respective results for the \proglang{R} packages are in Figure~\ref{fig:r-pkgs-pkg-counts} and Table~\ref{tab:r-packages-results-counts1}. The simulation performance with the \pkg{reda} functions is excellent. Performance with \pkg{simEd} and \pkg{IndTestPP} functions depends on the adequacy with which they approximate the target density. In this example, the approximation accuracy is not ideal for either package, but is somewhat worse for \pkg{IndTestPP}.

<<r-pkgs-pkg-counts, fig.cap="Theoretical (red) and empirical (black) cumulative distribution functions for event counts in the illustration example with the \\proglang{R} packages in Table~\\ref{tab:R-packages}. The unsigned area between the theoretical and empirical curves equals the Wasserstein-1 distance in Table~\\ref{tab:nhppp-results-counts}.", fig.height=8, fig.width=12, echo=FALSE>>=
epdf_r_pkgs_counts <- gridExtra::grid.arrange(
  grobs = lapply(1:nrow(r_pkgs_simulated_counts),
    plot_ecdf,
    simulated_counts = r_pkgs_simulated_counts
  ),
  ncol = 3
)
@




<<performance-r-packages-1, results='asis', echo=FALSE>>=
print(
  xtable::xtable(metrics_counts_r_pkgs[, 1:5],
    type = "latex",
    caption = "Simulated total number of events with the \\proglang{R} packages of Table~\\ref{tab:R-packages} for the illustration example. Equal tail $p$\\% CI: a confidence interval whose bounds are the $p/2$ and $(1-p/2)$ count percentiles of the respective cumulative distribution function.",
    label = "tab:r-packages-results-counts1",
    align = c("r", "p{1in}", "p{1in}", "p{1in}", "p{1in}", "p{1.1in}")
  ),
  math.style.negative = TRUE,
  sanitize.text.function = identity,
  sanitize.colnames.function = sanitize_strings_for_tables,
  sanitize.rownames.function = identity,
  scalebox = 0.77
)
@


\subsection{Event times}\label{sec:example-event-times}

We compared the theoretical and empirical distribution of event times for all $J=\Sexpr{n_simulations}$ event time draws. We calculated a goodness of fit statistic by binning realized times in $70$ bins and its $p$~value, by comparing the statistic against the $\chi^2_{69}$ distribution. We also calculated the $W_1$ distance between these distributions and its associated $p$~value.

Figure~\ref{fig:epdf-nhppp-pkg-times} and Table~\ref{tab:nhppp-gof-times} indicate excellent simulation performance with the \pkg{nhppp} functions.

Figure~\ref{fig:epdf-r-pkgs-times} and Table~\ref{tab:r-pkgs-gof-times} indicate excellent simulation performance with the \pkg{reda} functions. The simulation performance with the \pkg{simEd} and \pkg{IndTestPP} functions, which rely on approximations, is not as good.

<<define-functions-for-times, echo=FALSE, include=TRUE>>=
get_chi2_times <- function(x, Lambda) {
  n_breaks <- 70
  xmids <- hist(x, breaks = n_breaks, plot = FALSE)$mids # histogram time breaks
  E <- sapply(seq_along(xmids), function(i) {
    (Lambda(xmids[i + 1]) - Lambda(xmids[i])) / (Lambda(6 * pi) - Lambda(0))
  })
  E <- E[-length(E)]
  O <- sapply(seq_along(xmids), function(i) {
    sum(x >= xmids[i] & x <= xmids[i + 1]) / length(x)
  })
  O <- O[-length(O)]
  gof <- sum(((O - E)^2) / E)
  pval <- pchisq(gof, length(O) - 1, lower.tail = FALSE)
  return(list(gof = gof, pval = pval, O = O, E = E))
}
get_wasserstein_times <- function(O, E, tmp_seed = 123, B = 1000, n_breaks = NULL) {
  # empirical distribution PMF
  empirical_pmf <- matrix(O / sum(O), ncol = 1)
  theoretical_pmf <- matrix(E / sum(E), ncol = 1)

  cost_mat <- as.matrix(stats::dist(arrayInd(1:nrow(theoretical_pmf), .dim = nrow(theoretical_pmf))))
  w1 <- otinference::wassDist(a = empirical_pmf[, 1], b = theoretical_pmf[, 1], distMat = cost_mat, p = 1)

  if (is.null(n_breaks)) {
    n_breaks <- length(O)
  }
  current_seed <- .Random.seed
  set.seed(tmp_seed)
  pval <- otinference::binWDTest(x = empirical_pmf, y = theoretical_pmf, L = n_breaks, B = B)
  set.seed(current_seed)
  return(list(w1 = w1, pval = pval))
}

performance_metrics_times <- function(index, simulated_times, Lambda = L, digits = 3, tmp_seed = 123, B = 500, n_breaks = 5) {
  x <- simulated_times[[index]]
  x <- x[!is.na(x)]

  chi2 <- get_chi2_times(x = x, Lambda = Lambda)
  WD <- get_wasserstein_times(O = chi2$O, E = chi2$E, B = B, n_breaks = n_breaks)

  return(c(
    `Goodness of fit, $\\chi^2$ [$p$~value]` = paste0(pprint(chi2$gof, digits), " [", pprint(chi2$pval, digits, use_lt = T), "]"),
    `$W_1$ [$p$~value]` = paste0(pprint(WD$w1, digits, use_lt = T), " [", pprint(WD$pval, digits, use_lt = T), "]")
  ))
}

nhppp_pkg_times <- apply(nhppp_pkg_event_series, 1, unlist)

times_hist <- function(index, simulated_times, lambda = l, Lambda = L) {
  x <- simulated_times[[index]]
  x <- x[!is.na(x)]
  p <- ggplot(NULL) +
    geom_histogram(aes(x = x, y = after_stat(density)), bins = 50, color = "black", fill = "grey") +
    geom_line(aes(x = x, y = lambda(x) / (Lambda(6 * pi) - Lambda(0))), linewidth = 0.5, color = "red") +
    theme_bw() +
    theme(text = element_text(size = 15)) +
    labs(
      y = "Density", x = "Time",
      title = TeX(sanitize_strings_for_figures(names(simulated_times)[index]))
    )
  return(p)
}


times_ecdf <- function(index, simulated_times, lambda = l, Lambda = L) {
  x <- simulated_times[[index]]
  x <- x[!is.na(x)]
  p <- ggplot(NULL) +
    stat_ecdf(aes(x = x), linetype = "solid", color = "black") +
    geom_line(aes(x = x, y = Lambda(x) / (Lambda(6 * pi) - Lambda(0))), linewidth = 0.5, color = "red") +
    labs(x = "Count", y = "Cumulative density") +
    coord_cartesian(xlim = c(0, 6 * pi)) +
    ggtitle(TeX(sanitize_strings_for_figures(names(simulated_times)[index]))) +
    theme_bw() +
    theme(text = element_text(size = 15))
  return(p)
}
@


<<get-metrics-for-times, echo=FALSE, include=FALSE>>=
metrics_times_nhppp_pkg <- t(
  sapply(seq_along(nhppp_pkg_times),
    performance_metrics_times,
    simulated_times = nhppp_pkg_times,
    n_breaks = n_bins_wasserstein,
    B = n_samples_wasserstein
  )
)
rownames(metrics_times_nhppp_pkg) <- names(nhppp_pkg_times)

metrics_times_r_pkgs <- t(
  sapply(seq_along(r_pkg_times),
    performance_metrics_times,
    simulated_times = r_pkg_times,
    n_breaks = n_bins_wasserstein,
    B = n_samples_wasserstein
  )
)
rownames(metrics_times_r_pkgs) <- names(r_pkg_times)
@


<<performance-nhppp-pkg-times, results='asis', echo=FALSE>>=
print(
  xtable::xtable(metrics_times_nhppp_pkg,
    type = "latex",
    caption = "Goodness of fit of simulated event times with \\pkg{nhppp} functions for the example.",
    label = "tab:nhppp-gof-times"
  ),
  math.style.negative = TRUE,
  sanitize.text.function = sanitize_strings_for_tables,
  scalebox = 1
)
@

<<performance-r-pkg-times, results='asis', echo=FALSE>>=
print(
  xtable::xtable(metrics_times_r_pkgs,
    type = "latex",
    caption = "Goodness of fit of simulated event times with \\proglang{R} functions in Table~\\ref{tab:R-packages}.",
    label = "tab:r-pkgs-gof-times"
  ),
  math.style.negative = TRUE,
  sanitize.text.function = sanitize_strings_for_tables,
  scalebox = 1
)
@



<<epdf-nhppp-pkg-times,fig.cap="Simulated event times with \\pkg{nhppp}. Left column: histogram (gray) and theoretical distribution (red) of event times; right column: empirical (black) and theoretical (red) cumulative distribution function. The unsigned area between the empirical and cumulative distribution functions is the $W_1$ distance in Table~\\ref{tab:nhppp-gof-times}.", fig.width=8, fig.height=20, fig.align='center', out.height = "\\textheight", out.extra = "keepaspectratio=true", echo=FALSE>>=
epdf_nhppp_pkg_times <- gridExtra::grid.arrange(
  grobs = c(
    lapply(seq_along(nhppp_pkg_times),
      times_hist,
      simulated_times = nhppp_pkg_times
    ),
    lapply(seq_along(nhppp_pkg_times),
      times_ecdf,
      simulated_times = nhppp_pkg_times
    )
  ),
  as.table = FALSE,
  ncol = 2
)
@


<<epdf-r-pkgs-times,fig.cap="Simulated event times with the \\proglang{R} packages in Table~\\ref{tab:R-packages}. Left column: histogram (gray) and theoretical distribution (red) of event times; right column: empirical (black) and theoretical (red) cumulative distribution function. The unsigned area between the empirical and cumulative distribution functions is the $W_1$ distance in Table~\\ref{tab:r-pkgs-gof-times}.", fig.width=8, fig.height=20, fig.align='center', out.height = "\\textheight", out.extra = "keepaspectratio=true", echo=FALSE>>=
epdf_r_pkg_times <- gridExtra::grid.arrange(
  grobs = c(
    lapply(seq_along(r_pkg_times),
      times_hist,
      simulated_times = r_pkg_times
    ),
    lapply(seq_along(r_pkg_times),
      times_ecdf,
      simulated_times = r_pkg_times
    )
  ),
  as.table = FALSE,
  ncol = 2
)
@


\subsection{Time performance}\label{sec:example-time-performance}

\subsubsection{Time performance of nonvectorized functions}\label{sec:nonvectorized-functions}

To indicate time performance, we benchmarked functions by recording execution times when drawing a series of points (Figure~\ref{fig:comptimes_all_samples}). We also benchmarked functions for drawing the first-occurring event, because \pkg{nhppp} functions can sample the first time more efficiently when the inversion algorithm is used (Figure~\ref{fig:comptimes_one_sample}).

We provided functions with the arguments they need to run fastest. For example, functions that use the inversion or order statistics algorithm execute faster when the inverse function $\Lambda^{-1}(z)$ is provided, rather than numerically calculated, as shown in both Figures for the \pkg{nhppp} package. (Functions in other packages do not take $\Lambda(t)$ and $\Lambda^{-1}(z)$ arguments.) The fastest functions are \pkg{nhppp} functions that rely on the inversion or order statistics algorithms given $\Lambda^{-1}(z)$.

According to~\eqref{eq:thinning-efficiency}, the thinning algorithm has higher efficiency, and is expected to execute faster, for majorizer functions that envelop the intensity function more closely. Observe that $\lambda_{*a} \succ \lambda_{*c}$ and $\lambda_{*b} \succ \lambda_{*c}$ in Figure~\ref{fig:example-function-plot}. As expected, the execution times are indeed shorter for majorizer `c' compared to `b' in Figures~\ref{fig:comptimes_all_samples} and~\ref{fig:comptimes_one_sample}. However, the execution times are longer with majorizer `c' compared to `a' because \fct{draw\_intensity}, the function that uses constant majorizers, has a different implementation than \fct{draw\_intensity\_step}, the function that use piecewise constant majorizers. \fct{draw\_intensity} happens to be faster in this example, but this is not always the case.

In \pkg{nhppp}, functions that use the inversion or order statistics algorithms can exit earlier when only the first event is requested. This is not possible, however, for the thinning algorithm. This efficiency does not appear to be implemented in the other packages.




<<compute-times, echo=FALSE, include=FALSE, cache=TRUE>>=
comptimes_all_samples <- bench::mark(
  "(1a) nhppp: thinning (majorizer a)" = nhppp::draw_intensity(
    lambda = l, lambda_maj = l(6 * pi), range_t = c(0, 6 * pi), atmost1 = FALSE
  ),
  "(1b) nhppp: thinning (majorizer b)" = nhppp::draw_intensity_step(
    lambda = l, lambda_maj_vector = l_star(t_breaks[1:M]),
    times_vector = t_breaks, atmost1 = FALSE
  ),
  "(1c) nhppp: thinning (majorizer c)" = nhppp::draw_intensity_step(
    lambda = l, lambda_maj_vector = l_star2(t_breaks[1:M]),
    times_vector = t_breaks, atmost1 = FALSE
  ),
  "(1d) nhppp: inversion (with inverse)" =
    nhppp::draw_cumulative_intensity_inversion(
      Lambda = L, Lambda_inv = Li, range_t = c(0, 6 * pi), atmost1 = FALSE
    ),
  "(1e) nhppp: inversion (no inverse)" =
    nhppp::draw_cumulative_intensity_inversion(
      Lambda = L, Lambda_inv = NULL, range_t = c(0, 6 * pi), atmost1 = FALSE
    ),
  "(1f) nhppp: order statistics (with inverse)" =
    nhppp::draw_cumulative_intensity_orderstats(
      Lambda = L, Lambda_inv = Li, range_t = c(0, 6 * pi), atmost1 = FALSE
    ),
  "(1g) nhppp: order statistics (no inverse)" =
    nhppp::draw_cumulative_intensity_orderstats(
      Lambda = L, Lambda_inv = NULL, range_t = c(0, 6 * pi), atmost1 = FALSE
    ),
  "(2a) reda: inversion" = reda::simEvent(
    rho = l, origin = 0,
    endTime = 6 * pi, recurrent = TRUE, method = "inversion"
  ),
  "(2b) reda: thinning (majorizer a)" = reda::simEvent(
    rho = l, rhoMax = l(6 * pi), origin = 0,
    endTime = 6 * pi, recurrent = TRUE, method = "thinning"
  ),
  "(3a) IndTestPP: inversion" = IndTestPP::simNHPc(
    lambda =
      l(seq(0, 6 * pi, length.out = ceiling(6 * pi))), fixed.seed = NULL,
    algor = "Inversion"
  )$posNH,
  "(3b) IndTestPP: thinning (no majorizer)" = IndTestPP::simNHPc(
    lambda =
      l(seq(0, 6 * pi, length.out = ceiling(6 * pi))), fixed.seed = NULL,
    algor = "Thinning"
  )$posNH,
  "(4) simEd: thinning (majorizer a)" = simEd::thinning(
    maxTime = ceiling(6 * pi),
    intensityFcn = l, majorizingFcn = function(i) l(6 * pi), majorizingFcnType = NULL,
    seed = 2024, maxTrials = Inf, plot = FALSE
  ),
  check = FALSE, filter_gc = TRUE,
  max_iterations = n_simulations, memory = FALSE
)

comptimes_one_sample <- bench::mark(
  "(1a) nhppp: thinning (majorizer a)" = nhppp::draw_intensity(
    lambda = l, lambda_maj = l(6 * pi), range_t = c(0, 6 * pi), atmost1 = TRUE
  ),
  "(1b) nhppp: thinning (majorizer b)" = nhppp::draw_intensity_step(
    lambda = l, lambda_maj_vector = l_star(t_breaks[1:M]),
    times_vector = t_breaks, atmost1 = TRUE
  ),
  "(1c) nhppp: thinning (majorizer c)" = nhppp::draw_intensity_step(
    lambda = l, lambda_maj_vector = l_star2(t_breaks[1:M]),
    times_vector = t_breaks, atmost1 = TRUE
  ),
  "(1d) nhppp: inversion (with inverse)" =
    nhppp::draw_cumulative_intensity_inversion(
      Lambda = L, Lambda_inv = Li, range_t = c(0, 6 * pi), atmost1 = TRUE
    ),
  "(1e) nhppp: inversion (no inverse)" =
    nhppp::draw_cumulative_intensity_inversion(
      Lambda = L, Lambda_inv = NULL, range_t = c(0, 6 * pi), atmost1 = TRUE
    ),
  "(1f) nhppp: order statistics (with inverse)" =
    nhppp::draw_cumulative_intensity_orderstats(
      Lambda = L, Lambda_inv = Li, range_t = c(0, 6 * pi), atmost1 = TRUE
    ),
  "(1g) nhppp: order statistics (no inverse)" =
    nhppp::draw_cumulative_intensity_orderstats(
      Lambda = L, Lambda_inv = NULL, range_t = c(0, 6 * pi), atmost1 = TRUE
    ),
  "(2a) reda: inversion" = reda::simEvent(
    rho = l, origin = 0,
    endTime = 6 * pi, recurrent = FALSE, method = "inversion"
  ),
  "(2b) reda: thinning (majorizer a)" = reda::simEvent(
    rho = l, rhoMax = l(6 * pi), origin = 0,
    endTime = 6 * pi, recurrent = FALSE, method = "thinning"
  ),
  "(3a) IndTestPP: inversion" = IndTestPP::simNHPc(
    lambda =
      l(seq(0, 6 * pi, length.out = ceiling(6 * pi))), fixed.seed = NULL,
    algor = "Inversion"
  )$posNH[1],
  "(3b) IndTestPP: thinning (no majorizer)" = IndTestPP::simNHPc(
    lambda =
      l(seq(0, 6 * pi, length.out = ceiling(6 * pi))), fixed.seed = NULL,
    algor = "Thinning"
  )$posNH[1],
  "(4) simEd: thinning (majorizer a)" = simEd::thinning(
    maxTime = ceiling(6 * pi),
    intensityFcn = l, majorizingFcn = function(i) l(6 * pi), majorizingFcnType = NULL,
    seed = 2024, maxTrials = Inf, plot = FALSE
  )[1],
  check = FALSE, filter_gc = TRUE,
  max_iterations = n_simulations, memory = FALSE
)

plot_comptimes <- function(benchdata, title = "") {
  autoplot(benchdata, type = c("ridge")) +
    labs(x = "", y = "") +
    ggtitle("") +
    theme(
      axis.text.x = element_text(size = 8),
      axis.text.y = element_text(size = 8)
    ) +
    # scale_x_log10(limits = c(10^-6, 0.150), expand = c(0, 0)) +
    scale_y_discrete(
      limits = rev,
      labels = function(x) stringr::str_wrap(sanitize_strings_for_figures(x), width = 45)
    ) +
    annotate("text",
      x = benchdata$median,
      y = c(length(benchdata$median):1),
      label = gsub(" ", "", benchdata$median, fixed = TRUE),
      size = 3, vjust = 1.5, hjust = 0.5
    ) +
    ggtitle(title) +
    theme(
      axis.text.x = element_text(size = 15),
      axis.text.y = element_text(size = 15),
      plot.margin = margin(t = 0, r = 1, b = 0, l = 0, "cm")
    )
}
@

<<comptimes_all_samples,fig.cap="Computation times when drawing all events in interval. Unit \\texttt{us} is $\\mu s$ (microsecond).", fig.height=7, fig.width=10, echo=FALSE, warning=FALSE, message=FALSE>>=
plot_comptimes(comptimes_all_samples)
@


<<comptimes_one_sample,fig.cap="Computation times when drawing the first event in interval. Unit \\texttt{us} is $\\mu s$ (microsecond).", fig.height=7, fig.width=10, echo=FALSE, warning=FALSE, message=FALSE>>=
plot_comptimes(comptimes_one_sample)
@


\newpage
\subsubsection{Time performance of vectorized functions}\label{sec:vectorized-functions}

<<vectorized-vs-not, include=FALSE, echo=FALSE, cache=TRUE>>=
non_vec_fun <- function(x = n_simulations, atmost1 = FALSE) {
  lapply(X = 1:x, FUN = function(x) draw_sc_step_regular(lambda_vector = l_star(t_breaks[1:M]), range_t = c(0, 6 * pi), atmost1 = atmost1))
}
Lmat <- matrix(rep(l_star(t_breaks[1:M]), n_simulations), nrow = n_simulations, byrow = TRUE)
Lmat <- nhppp:::mat_cumsum_columns(Lmat)

comptimes_vec <- bench::mark(
  nonvectorized_all = non_vec_fun(x = n_simulations, atmost1 = FALSE),
  vectorized_all = vdraw_sc_step_regular(Lambda_matrix = Lmat, range_t = c(0, 6 * pi), atmost1 = FALSE),
  nonvectorized_one = non_vec_fun(x = n_simulations, atmost1 = TRUE),
  vectorized_one = vdraw_sc_step_regular(Lambda_matrix = Lmat, range_t = c(0, 6 * pi), atmost1 = TRUE),
  check = FALSE, filter_gc = TRUE,
  max_iterations = n_simulations, memory = FALSE
)
@

In \proglang{R}, `vectorized' computation, where operations are done in columns, is faster than using \code{for} loops or \fct{apply} functions. As of this writing, the only vectorized \pkg{nhppp} function is \fct{vdraw\_sc\_step\_regular}, which samples from an array of piecewise constant intensity functions provided that all constant-rate time intervals have equal length (are `regular'). It is the vectorized analogue of \fct{draw\_sc\_step\_regular}.

We compared the execution speed of non-vectorized and vectorized functions for sampling $\Sexpr{n_simulations}$ times from the piecewise constant `b' majorizer ($\lambda_{*b}$) in Figure~\ref{fig:example-function-plot}.
%
When drawing only the earliest event, the vectorized function is approximately
$\Sexpr{pprint(as.numeric(comptimes_vec[[3,"median"]]) / as.numeric(comptimes_vec[[4,"median"]]), 0) }$ times faster than the non-vectorized function ($\Sexpr{pprint(as.numeric(comptimes_vec[[4,"median"]]) *10^6/n_simulations )} \mu s$ versus $\Sexpr{pprint(as.numeric(comptimes_vec[[3,"median"]]) *10^6/n_simulations )} \mu s$ per sample drawn.)
%
However, when drawing all events, the order changes: the vectorized function is approximately $\Sexpr{pprint(as.numeric(comptimes_vec[[2,"median"]]) / as.numeric(comptimes_vec[[1,"median"]]), 1) }$ times slower than the non-vectorized function ($\Sexpr{pprint(as.numeric(comptimes_vec[[2,"median"]]) *10^3/n_simulations )} ms$ versus $\Sexpr{pprint(as.numeric(comptimes_vec[[1,"median"]]) *10^3/n_simulations )} ms$ per set of event times drawn.) The reversal in apparent performance is because the vectorized function is not optimized for large numbers of events per draw. (The expected number of events with $\lambda_{*b}$ in $(0, 6\pi]$ is
$\Sexpr{pprint(Lmat[1,M], 2)}$.)


%% -- Summary/conclusions/discussion -------------------------------------------

\section{Summary and next developments} \label{sec:summary}

The \pkg{nhppp} is developed to aid the simulation of NHPPPs from general intensity or cumulative intensity functions. Its claimed advantages are that (i) it has functions that can simulate correctly from a target density, not just from an approximation; (ii) can sample conditional on observing at least one event; (iii) can use user provided random number stream objects; and it is fast. The current version includes only one vectorized function, for sampling from regular-spaced piecewise constant intensity functions. In future releases we will vectorize additional functions, possibly also porting a \proglang{C++} dynamically linked library that implements the algorithms and functions we described.


%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details and credits}

\proglang{R}~\Sexpr{paste0(version$major, ".",version$minor)}~\citep{R-program}
was used for all analyses.
Packages
\pkg{xtable}~{\Sexpr{utils::packageVersion("xtable")}}~\citep{xtable-package} and
\pkg{knitr}~{\Sexpr{utils::packageVersion("knitr")}}~\citep{knitr-package}
were used for automatic report generation. Packages
\pkg{ggplot2}~{\Sexpr{utils::packageVersion("ggplot2")}}~\citep{ggplot2-package},
\pkg{ggridges}~{\Sexpr{utils::packageVersion("ggridges")}}~\citep{ggridges-package}, and
\pkg{latex2exp}~{\Sexpr{utils::packageVersion("latex2exp")}}~\citep{latex2exp-package}
were used for plot generation and \LaTeX formatting.
Packages
\pkg{bench}~{\Sexpr{utils::packageVersion("bench")}}~\citep{bench-package},
\pkg{rstream}~{\Sexpr{utils::packageVersion("rstream")}}~\citep{rstream-package},
\pkg{otinference}~{\Sexpr{utils::packageVersion("otinference")}}~\citep{otinference-package}, and
\pkg{parallel}~{\Sexpr{utils::packageVersion("parallel")}}
were used in the examples and the analyses.

All computations were done on an Apple M1 Max machine with 64 megabytes of random access memory.
%
\proglang{R} itself
and all aforementioned packages are available from the Comprehensive
\proglang{R} Archive Network (CRAN) at
\url{https://CRAN.R-project.org/}.


\section*{Acknowledgments}
This work was funded from grant U01CA265750 from the National Cancer Institute.
We thank the investigators of the Cancer Incidence and Surveillance Modeling Network (CISNET)
Bladder Cancer Site Stavroula Chrysanthopoulou, Jonah Popp, Fernando Alarid-Escudero,
Hawre Jalal, and David Garibay for useful discussions.

%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{refs}


%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".

\newpage
\begin{appendix}

\section{Piecewise constant majorizer functions}\label{app:piecewise_majorizer}

Let $\lambda(t)$ be a $K$-Lipschitz continuous intensity function, i.e., an intensity function where $|(\lambda(b) - \lambda(a))| \le K|b-a|$, with $K$ known. Algorithm~\ref{alg:lambda_majorizer} finds a piecewise constant majorizing function $\lambda_*(t)$. Starting from a partition of the time interval in time steps (not necessarily equal) it finds an upper bound for $\lambda$ within the each partition.

If $\lambda(t)$ is monotonic, the least upper bound (supremum) is always found at the extremes of the interval and no knowledge of $K$ is required.

The algorithm should be started with a good partitioning of the time interval. In practice, it is generally easy to specify equispaced intervals that are fine enough and impose little computational penalty for the application.

The \pkg{nhppp}
function \fct{get\_step\_majorizer} implements Algorithm~\ref{alg:lambda_majorizer}. Functions \fct{ppp\_t\_step} and \fct{draw\_intensity\_step} expect the majorizer function values as an argument.
<<app-majorizer>>=
get_step_majorizer(
  fun = abs, breaks = -5:5, is_monotone = FALSE,
  K = 1
)
@


\input{algorithms/alg_majorizer}

\newpage
\section{Conditional sampling from NHPPPs} \label{app:conditional_sampling}
Algorithm~\ref{alg:NHPPP_conditional} is a direct modification of the order statistics Algorithm~\ref{alg:NHPPP_order_stats} (Line 1 modified, in red font).
To sample exactly $m$ points, change line 1 of Algorithm~\ref{alg:NHPPP_conditional} to
\begin{center}
$N \gets m$.
\end{center}
To sample up to $k$ earliest points, replace line 11 with in Algorithm~\ref{alg:NHPPP_conditional} with
\begin{center}
\textbf{return} {$\{Z_{(i)} \ | \ i \le k, Z_{(i)} \in \mathcal{Z}\}$}.
\end{center}

\input{algorithms/alg_nhppp_conditional}







\end{appendix}
\newpage
%% -----------------------------------------------------------------------------


\end{document}
