---
title: "vignette-NHPPP"
author: "Thomas A. Trikalinos, Yuliia Sereda" 
date: "2022-03-31"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignette-NHPPP}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

his package implements two approaches to draw random event times from a 
non-homogeneous Poisson Point Process (NHPPP) using the native random number generator 
or a provided `RngStream` object. 

# Notation 
A NHPPP has a time-varying non-negative rate $\lambda(t) \ge 0$ for time $t$ in some interval of interest $\mathcal{T} = (a, b)$. We assume $\lambda$ to be continuous in $\mathcal{T}$. Write $\lambda^*$ for the supremum of $\lambda$ over $\mathcal{T}$. 

Define $\Lambda(t) = \int_0^t \lambda(s) \ \text{d}s$ to be the integrated rate in the interval $(0, t)$. By construction $\Lambda$ is a continuous positive monotone function. The integrated rate over an arbitrary interval $(a, t)$ is obtained as $\Lambda(t) - \Lambda(a) = \int_a^t \lambda(s) \ \text{d}s$. 

Write $\Lambda^{-1}(z)$ for the inverse function of $\Lambda$, i.e., 
$\Lambda^{-1}\big(\Lambda(t)\big) = t$.

The two approaches are: 

1. The *inversion approach*, implemented in `nhppp_t_cumulative_intensity_inversion()`.  

    *TODO*: **Expand**. The best case for this function requires $\Lambda$, $\Lambda^{-1}$, and the evaluations $\Lambda(a), \Lambda(b)$, which are provided as options `Lambda`, `Lambda_inv`, and `range_L`, respectively. 
    
2. The *thinning approach*, implemented in `nhppp_t_intensity()`.  

    *TODO*: **Expand**. The best case for this function requires $\lambda$ and $\lambda_u$, provided as options `lambda` and `lambda_max`, respectively.


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = TRUE
)
# withr::local_preserve_seed()
set.seed(1234)
```

Load packages for NHPPP sampling and tests. 
```{r, setup}
library(nhppp) # NHPPP in C++
library(reda) # Alternative NHPPP packages - start
library(IndTestPP)
library(NHPoisson)
library(poisson)
library(PtProcess)
library(simEd) # Alternative NHPPP packages - end
library(rstream) # Random streams
library(tidyverse) # Data manipulation
library(twosamples) # Wasserstein Distance
library(parallel) # parallel computing
```

# Simulating from NHPPP

Consider an example of sinusoidal intensity function with an exponential amplitude and its antiderivative, where $r=0.2$ and $t \in [0, 6\pi)$. In thinning algorithm, the rate of the homogeneous Poisson PP which dominated $\lambda(t)$ was $\lambda_u=\sup_{0 \leq t \leq 6\pi} \lambda(t) = 43.38$.
```{r}
r <- 0.2
tp <- c(0, 6 * pi)
l <- function(t) { # lambda(t)
  exp(r * t) * (1 + sin(t))
}
L <- function(t) { # Lambda(t)
  (exp(r * t) * (r * sin(t) - cos(t))) / (r^2 + 1) + exp(r * t) / r
}
l_sup <- max(sapply(seq(tp[1], tp[2], length.out = 10^4), l)) # lambda(t) supremum
```

Load the `rstream` package and create an `RngStream` object. 
```{r}
S <- new("rstream.mrg32k3a")
rstream::rstream.reset(S)
```

From NHPPP definition, the number of events in the interval of interest is a Poisson random variable with a parameter $\Lambda(6\pi) - \Lambda(0) = 171.13$ representing the mean and variance. 
```{r}
mu <- L(tp[2]) - L(tp[1])
mu
```

To generate one NHPPP sequence, we can use `nhppp::nhppp_t_cumulative_intensity_inversion()` or `nhppp::nhppp_t_intensity()` functions:
```{r}
t1 <- nhppp::nhppp_t_cumulative_intensity_inversion(Lambda = L, range_t = tp, rng_stream = S, only1 = FALSE)
str(t1)
```

```{r}
t2 <- nhppp::nhppp_t_intensity(
  lambda = l, lambda_maj = l_sup,
  range_t = tp, rng_stream = S, only1 = FALSE
)
str(t2)
```

The argument `only1 = FALSE` simulates recurrent events and returnes all times within the interval given $\lambda(t)$. If interested in the time to the first event or non-recurrent events, pass `only1 = TRUE`.
```{r}
nhppp::nhppp_t_cumulative_intensity_inversion(Lambda = L, range_t = tp, rng_stream = S, only1 = TRUE)
```

```{r}
nhppp::nhppp_t_intensity(
  lambda = l, lambda_maj = l_sup,
  range_t = tp, rng_stream = S, only1 = TRUE
)
```

# Algorithm bias

We performed several tests to ensure that our implementation simulates from the target intensity and it's computationally efficient. The code below compares simulations with theoretical distribution with respect to the number of events and distribution of times.  

First, we generated $10^4$ NHPPP trajectories.
```{r}
# List of trajectories - inversion
nsim <- 10^4
nhppp_InvThin <- function() {
  Inversion <- nhppp::nhppp_t_cumulative_intensity_inversion(
    Lambda = L,
    range_t = tp,
    rng_stream = S,
    only1 = FALSE
  )
  
  Thinning <- nhppp::nhppp_t_intensity(
    lambda = l,
    lambda_maj = l_sup,
    range_t = tp,
    rng_stream = S,
    only1 = FALSE
  )
  return(list(
    Inversion = Inversion,
    Thinning = Thinning
  ))
}
```

```{r}
nhppp_traj <- parallel::mcmapply(
  function(i) {
    nhppp_InvThin()
  },
  1:nsim,
  mc.cores = parallel::detectCores() - 1
)
```

The expected number of events in the interval is $Pois(171)$. If algorithms work well, the shapes of count distributions should be aligned.

```{r}
# Empirical count distribution
nhppp_count <- apply(nhppp_traj, 2, function(x) sapply(x, length))

# Target count density Pois(171)
t <- as.integer(seq(qpois(0.0001, mu), qpois(0.9999, mu), length.out = nsim))
d <- dpois(t, mu)

# Target vs. simulated count distributions
p1 <- ggplot(NULL) +
  geom_line(aes(x = t, y = d, linetype = "Theoretical", color = "Theoretical")) +
  geom_density(aes(x = nhppp_count[1, ], linetype = "Inversion", color = "Inversion")) +
  geom_density(aes(x = nhppp_count[2, ], linetype = "Thinning", color = "Thinning")) +
  labs(x = "Count", y = "Density", linetype = "") +
  scale_linetype_manual(
    name = "",
    values = c(
      "Inversion" = 3,
      "Thinning" = 2,
      "Theoretical" = 1
    )
  ) +
  scale_colour_manual(
    name = "",
    values = c(
      "Inversion" = "#0072B2",
      "Thinning" = "#D55E00",
      "Theoretical" = "#999999"
    )
  ) +
  theme_bw()
# ggsave("count_densities.pdf", p1, width = 10, height = 7, units = "cm")
p1
```

To compare expected and generated count distributions, we calculated several metrics with respect to the number of events in interval:
  1. Mean absolute bias: $Bias_{N} = \frac{1}{K} \sum_k^K X_k - \Lambda[a,b]$
  2. Mean relative bias: $RBias_N = \frac{Bias_N} {\Lambda[a, b]}$
  3. Variance absolute bias: $Bias_{V} =  \frac{1}{K}(X_k - \frac{1}{K} \sum_k^K X_k)^2 - \Lambda[a,b]$
  4. Variance relative bias: $RBias_V = \frac{Bias_V} {\Lambda[a, b]}$
  5. Coverage of parameter $\Lambda[a,b]$. We compared $p$\% confidence interval $CI_p$ from theoretical density $Pois(\Lambda[a,b])$ and simulations, where $p \in (95, 90, 75, 50)$.
  6. Goodness of Fit statistic: ${GOF = \sum_{x \in [0, x_U]} \frac{(O_x - E_x)^2}{E_x}}$ for $x \in X_L, \dots, X_U$, where $O_x$ and $E_x$ represent percentiles of simulated and theoretical distribution respectively. $X_L, X_U$ are the 0.01 and 99.9 percentiles of the Poisson with parameter $\Lambda[a, b]$. $GOF$ is $\chi^2_{X_U-X_L}$. $GOF$ values close to 1 imply a good fit.  
  7. The Wasserstein test, which compares simulated and theoretical distribution by Wasserstein distance $d$ between the two. The test p-value is calculated by randomly resampling two samples of the same size using the combined sample. $d$ values close to 0 imply a good fit. 
  
Both algorithms showed satisfactory fit.
```{r}
# Metrics with respect to the number of events in interval
bias_count <- function(x, digits) {
  # Mu bias
  mean_obs <- mean(x)
  bN <- mean(x - mu)
  bNr <- (bN / mu) * 100 # as percentage

  # Variance bias
  var_obs <- var(x)
  bV <- var(x) - mu
  bVr <- (bV / mu) * 100 # as percentage

  # Observed vs Expected count
  pr1 <- seq(0.0001, 0.999, length.out = 100)
  E <- qpois(pr1, mu)
  O <- quantile(x, pr1)

  # Goodness of Fit
  GOF <- sum(((O - E)^2) / E)
  GOF_pval <- pchisq(GOF, length(O) - 1, lower.tail = F)

  # Coverage
  pr2 <- c(0.025, 0.05, 0.125, 0.25, 0.75, 0.875, 0.95, 0.975)
  th <- qpois(pr2, mu) # theoretical percentiles
  emp <- as.integer(quantile(x, pr2)) # empirical percentiles
  names(th) <- names(emp) <- paste0(pr2 * 100, "%")

  # Wasserstein distance and p-value
  ed <- density(x)
  set.seed(784)
  td <- dpois(as.integer(ed[[1]]), mu)
  wd <- twosamples::wass_test(ed$y, td)

  return(c(
    mu = round(mean_obs, digits),
    BiasN = round(bN, digits),
    RBiasN = round(bNr, digits),
    V = round(var_obs, digits),
    BiasV = round(bV, digits),
    RBiasV = round(bVr, digits),
    GOF = paste0(round(GOF, digits), " [", round(GOF_pval, digits), "]"),
    WD = paste0(round(wd[1], digits), " [", round(wd[2], digits), "]"),
    th_95 = paste0(th[1], "; ", th[8]),
    th_90 = paste0(th[2], "; ", th[7]),
    th_75 = paste0(th[3], "; ", th[6]),
    th_50 = paste0(th[4], "; ", th[5]),
    emp_95 = paste0(emp[1], "; ", emp[8]),
    emp_90 = paste0(emp[2], "; ", emp[7]),
    emp_75 = paste0(emp[3], "; ", emp[6]),
    emp_50 = paste0(emp[4], "; ", emp[5])
  ))
}

# Results
metrics <- apply(nhppp_count, 1, bias_count, digits = 3)
metrics
# Latex table
# print(xtable::xtable(metrics, type = "latex"), file = "metrics.tex")
```

Then, we compared algorithm bias in released R packages simulating from NHPPP. We focused on the following packages: `reda`, `NHPoisson`, `poisson`, `IndTestPP` and `simEd`. `NHPoisson`, `poisson`, `IndTestPP` require $\lambda(t)$ as a numeric vector, while `reda` and `simEd` can pass functions. Other packages exist but they were not applicable for our example. For instance, `PtProcess` package can simulate different point processes, including NHPPP and processes conditional on previous history. However, the package limits the functional form of intensity function and implementation of $\lambda(t) = exp(rt)(1 + sin(t))$ was not possible. Another package, `spatstat.random` generates two-dimensional NHPPP, which was not comparable with other packages. 

```{r}
# Function to generate NHPPP trajectories from released R packages with our parameters
reNHpoInsi <- function() {
  # reda
  reda_thinning <- reda::simEvent(
    rho = l,
    origin = tp[1], endTime = tp[2],
    recurrent = T,
    method = "thinning"
  )
  reda_thinning_lmax <- reda::simEvent(
    rho = l,
    rhoMax = 1.1 * l_sup,
    origin = tp[1], endTime = tp[2],
    recurrent = T,
    method = "thinning"
  )
  reda_inversion <- reda::simEvent(
    rho = l,
    origin = tp[1], endTime = tp[2],
    recurrent = T,
    method = "inversion"
  )

  # NHPoisson returns indexes of moments of events instead of actual times!
  # The length of times n=570 returns 171 events on average (calibrated)
  t_seq_1 <- seq(tp[1], tp[2], length.out = 570)
  NHPoisson_i <- NHPoisson::simNHP.fun(lambda = l(t_seq_1))$posNH
  NHPoisson_inversion <- t_seq_1[NHPoisson_i]

  # poisson: fixed count:  num.events = L(tp[2]) - L(tp[1])
  poisson_thinning <- poisson::nhpp.event.times(
    rate = l_sup, # calibrated rate = 28
    num.events = floor(L(tp[2]) - L(tp[1])),
    prob.func = function(t) {
      l(t) / l_sup
    },
    num.sims = 1, t0 = 0
  )

  # PtProcess: no applicable functional form

  # IndTestPP
  # t length determines the length of the observed period
  t_seq_2 <- seq(tp[1], tp[2], length.out = tp[2])
  IndTestPP_thinning <- IndTestPP::simNHPc(lambda = l(t_seq_2), fixed.seed = NULL, algor = "Thinning")$posNH
  hist(IndTestPP_thinning)
  IndTestPP_inversion <- IndTestPP::simNHPc(lambda = l(t_seq_2), fixed.seed = NULL, algor = "Inversion")$posNH

  # simEd
  simEd_thinning <- simEd::thinning(
    maxTime = ceiling(tp[2]), # must be a positive integer value greater than or equal to 1
    intensityFcn = l,
    majorizingFcn = NULL,
    majorizingFcnType = NULL,
    seed = NA,
    maxTrials = Inf,
    plot = F
  )

  simEd_thinning_lmax <- simEd::thinning(
    maxTime = ceiling(tp[2]),
    intensityFcn = l,
    majorizingFcn = function(x) {
      1.2 * l_sup
    },
    majorizingFcnType = NULL,
    seed = NA,
    maxTrials = Inf,
    plot = F
  )
  return(list(
    "reda thinning" = reda_thinning,
    "reda thinning with supremum" = reda_thinning_lmax,
    "reda inversion" = reda_inversion,
    "NHPoisson inversion" = NHPoisson_inversion,
    "poisson thinning" = poisson_thinning,
    "IndTestPP thinning" = IndTestPP_thinning,
    "IndTestPP inversion" = IndTestPP_inversion,
    "simEd thinning" = simEd_thinning,
    "simEd thinning with supremum" = simEd_thinning_lmax
  ))
}
```

```{r}
# Generate NHPPP trajectories from released packages
# We use cluster parallel computing to speed up replications
cl <- parallel::makeCluster(parallel::detectCores() - 1)
parallel::clusterExport(cl, c("reNHpoInsi", "l", "L", "tp", "r", "l_sup"))
reNHpoInsi_traj <- parallel::parSapply(cl, X = 1:10^3, FUN = function(i) {
  i <- reNHpoInsi()
})
```

NHPPP generation in the package `poisson` requires a user to define a number of generated times. The distribution of the number of events is not random. Thus, we excluded this package from count metrics. 
```{r}
# Bias in released packages
# Distributions of event count
reNHpoInsi_count <- apply(t(reNHpoInsi_traj), 1, function(x) sapply(x, length))
metrics2 <- apply(reNHpoInsi_count, 1, bias_count, digits = 3)
metrics2[, 1:4]
metrics2[, 6:9]
```

We compared normalized intensity function over time, $\frac{\lambda{t}}{\Lambda[a,b]}$ and area one histogram of event times for all $K=10^4$ event time draws. For the above plots, we calculated Wasserstein distance between the empirical (histograms) and theoretical (intensity function) density and Goodness of Fit ($GOF_t$) measure. $GOF_t$ compares observed and expected count of events across $m-th$ histogram bins, where $Expected_m = \Lambda[t_m, t_{m+1}]$ and $GOF_t \sim \chi^2_{M-1}$ with $M$ equal to the total number of bins. 

```{r}
# Metrics with respect to the event times
bias_times <- function(a, traj) {
  x <- traj[[a]]
  Name <- names(traj)
  # Wasserstein distance and p-value
  td <- density(x)$y
  ed <- l(density(x)$x) / mu
  wd <- round(twosamples::wass_test(td, ed), 3)
  # GOF and p-value
  b <- 70 #
  xmids <- hist(x, breaks = b, plot = F)$mids # histogram time breaks
  E <- sapply(seq_along(xmids), function(i) {
    (L(xmids[i + 1]) - L(xmids[i])) / mu
  })
  E <- E[-length(E)]
  O <- sapply(seq_along(xmids), function(i) {
    sum(x >= xmids[i] & x <= xmids[i + 1]) / length(x)
  })
  O <- O[-length(O)]
  GOF <- round(sum(((O - E)^2) / E), 3)
  GOF_pval <- round(pchisq(GOF, length(O) - 1, lower.tail = F), 3)

  # Target density vs area 1 histogram of random variates
  p <- ggplot(NULL) +
    geom_histogram(aes(x = x, y = ..density..),
      bins = b, color = "black", fill = "grey"
    ) +
    geom_line(aes(x = x, y = l(x) / mu), size = 1.2, color = "#0072B2") +
    theme_bw() +
    labs(y = "Density", x = "Time", title = Name[a]) +
    annotate(
      geom = "text", x = 4, y = 0.22,
      label = paste0(
        "Wasserstein d=", wd[[1]], ", p=", wd[[2]],
        "\n",
        "GOF=", GOF, ", p=", GOF_pval
      ), size = 2.5
    )

  return(p)
}
```

```{r}
# Metrics with respect to the event times: nhppp package
nhppp_times <- apply(nhppp_traj, 1, unlist)
p2 <- do.call(
  gridExtra::grid.arrange,
  c(lapply(seq_along(nhppp_times), bias_times, traj = nhppp_times), ncol = 2)
)
# ggsave("bias_times.pdf", p2, width = 7, height = 7, units = "cm")
p2
```

```{r}
# Bias metrics with respect to event times in functions sampling N events
n <- 10^4
nhppp_times_n <- list(
  "Inversion [fixed size]" = nhppp::nhppp_n_cumulative_intensity(size = n, 
                                             Lambda = L,
                                             range_t = tp,
                                             rng_stream = S,
                                             only1 = T),
  "Thinning [fixed size]" = nhppp::nhppp_n_intensity(size = n,
                                   lambda = l,
                                   lambda_maj = l_sup,
                                   range_t = tp,
                                   rng_stream = S)
)

p2_1 <- do.call(
  gridExtra::grid.arrange,
  c(lapply(seq_along(nhppp_times_n), bias_times, traj = nhppp_times_n), ncol = 2)
)
# ggsave("bias_times_n_functions.pdf", p2_1, width = 7, height = 7, units = "cm")
p2_1
```


```{r}
# Metrics with respect to the event times: released packages
reNHpoInsi_times <- apply(reNHpoInsi_traj, 1, unlist)
# Name <- dimnames(reNHpoInsi_traj)[[1]]
p2_1 <- do.call(
  gridExtra::grid.arrange,
  c(lapply(seq_along(reNHpoInsi_times), bias_times, traj = reNHpoInsi_times), ncol = 2)
)
# ggsave("bias_times2.pdf", p2_1, width = 20, height = 25, units = "cm")
p2_1
```
```{r}
# Expected time to event: parametric distributions vs. nhppp
# Exponential (rate = 1)
r <- 1
mu_exp <- 1 / r
# Gompertz (shape = 0.1, scale = 1)
eta <- 0.1
b <- 1
Ei <- function(eta, v) {
  integrate( # exponential integral function
    f = function(v) {
      exp(-v) / v
    },
    lower = -eta,
    upper = Inf
  )$value
}
mu_gompertz <- (1 / b) * exp(eta) * Ei(-eta)
# Gamma (shape = 1, scale = 2)
k <- 1
theta <- 2
mu_gamma <- k * theta

l_list <- list(
  l_exp = function(t) {
    r * exp((-r) * t)
  },
  l_gomp = function(t) {
    b * eta * exp(eta + b * t - eta * exp(b * t))
  },
  l_gamma = function(t) {
    (t^(k - 1) * exp(-t / theta)) / (gamma(k) * theta^k)
  }
)

# Parametric distributions: plots
t_seq <- seq(0, 20, by = 1)
par(mfrow = c(1, 3))
plot(t_seq, l_list[[1]](t_seq), type = "l", xlab = "time", ylab = "density", main = "Exponential")
plot(t_seq, l_list[[2]](t_seq), type = "l", xlab = "time", ylab = "density", main = "Gompertz")
plot(t_seq, l_list[[3]](t_seq), type = "l", xlab = "time", ylab = "density", main = "Gamma")

# Empirical distributions
nsim <- 10^4
tp <- c(0, 20)

inversion_w_integration <- function(lambda, range_t, only1 = FALSE) {
  Lambda_int <- function(t) {
    nhppp::simpson_num_integr(f = lambda, a = range_t[1], b = t, n = 50)
  }
  nhppp::nhppp_t_cumulative_intensity_inversion(
    Lambda = Lambda_int, range_t = range_t,
    rng_stream = NULL, only1 = only1
  )
}

param_inv <- replicate(
  nsim,
  lapply(
    seq_along(l_list),
    function(i) {
      inversion_w_integration(
        lambda = l_list[[i]],
        range_t = tp,
        only1 = F
      )
    }
  )
)
param_thin <- replicate(
  nsim,
  lapply(
    seq_along(l_list),
    function(i) {
      nhppp::nhppp_t_intensity(
        lambda = l_list[[i]],
        range_t = tp,
        only1 = F
      )
    }
  )
)

# Expected time to event between parametric distributions and nhppp
cbind(
  theoretical = c(mu_exp = mu_exp, mu_gompertz = mu_gompertz, mu_gamma = mu_gamma),
  inversion = unlist(lapply(apply(param_inv, 1, unlist), mean)),
  thinning = unlist(lapply(apply(param_thin, 1, unlist), mean))
)
```


# Computational time

We calculated average computational times across $10^4$ iterations for a simple linear intensity function $\lambda(t) = 2t$ with $t\in [0,10)$, where the cumulative intensity is $\Lambda(t) = t^2$, and the inversion function is $\Lambda^{-1}(z) = \sqrt{z}$. 

```{r}
# Computational time

# Parameters -- Computations
l <- function(t) {
  2 * t
}
L <- function(t) {
  t^2
}
Li <- function(z) {
  sqrt(z)
}
tp <- c(0, 10)
t <- seq(tp[1], tp[2], length.out = 1000)
l_max <- max(sapply(t, l))
n_times <- 10^4
```

The \textit{best-arguments scenario} is the one with the least number of steps, which is an inversion algorithm with known closed-form solutions for $\Lambda(t)$ and $\Lambda^{-1}(t)$, and thinning with pre-defined $\lambda_u$. \textit{Worse-arguments scenario} is a common case when we pass $\lambda(t)$ to both methods, i.e., thinning with unknown supremum or majorizing function and inversion with numerical solutions for $\Lambda(t)$ and $\Lambda^{-1}(t)$. In addition to the variation in intensity arguments, we calculated time penalties for generating all event times within the interval vs. time to the next event and for using a pointer to the random number generator. Finally, we compared computational times for implementations in \texttt{R} and \texttt{C++}. The latter helps to address overheads of recursive functions, a known bottleneck in \texttt{R}. \texttt{Rcpp} package compiles \texttt{C++} code in \texttt{R} environment. 

```{r}
# Worse arguments - inversion with integration
inversion_w_integration <- function(lambda, range_t, only1 = FALSE) {
  Lambda_int <- function(t) {
    nhppp::simpson_num_integr(f = lambda, a = range_t[1], b = t, n = 50)
  }
  nhppp::nhppp_t_cumulative_intensity_inversion(
    Lambda = Lambda_int, range_t = range_t,
    rng_stream = NULL, only1 = only1
  )
}
```

```{r}
comptimes <- bench::mark(
  "(1a) R: Inversion (best args, all samples, no substream)" = nhppp::nhppp_t_cumulative_intensity_inversion(
    Lambda = L, Lambda_inv = Li, range_t = tp, rng_stream = NULL, only1 = F
  ),
  "(1b) R: Inversion (best args, all samples, substream)" = nhppp::nhppp_t_cumulative_intensity_inversion(
    Lambda = L, Lambda_inv = Li, range_t = tp, rng_stream = S, only1 = F
  ),
  "(1c) R: Inversion (best args, one sample, no substream)" = nhppp::nhppp_t_cumulative_intensity_inversion(
    Lambda = L, Lambda_inv = Li, range_t = tp, rng_stream = NULL, only1 = T
  ),
  "(1d) R: Inversion (worse args, all samples, no substream)" = inversion_w_integration(lambda = l, range_t = tp, only1 = FALSE),
  "(2a) R: Thinning (best args, all samples, no substream)" = nhppp::nhppp_t_intensity(
    lambda = l, lambda_maj = l_max, range_t = tp, rng_stream = NULL, only1 = F
  ),
  "(2b) R: Thinning (best args, all samples, no substream, oblique majorizer)" = nhppp::nhppp_t_intensity(
    lambda = l, lambda_maj = c(1, 2), range_t = tp, rng_stream = NULL, only1 = F
  ),
  "(2c) R: Thinning (best args, all samples, substream)" = nhppp::nhppp_t_intensity(
    lambda = l, lambda_maj = l_max, range_t = tp, rng_stream = S, only1 = F
  ),
  "(2d) R: Thinning (best args, one sample, no substream)" = nhppp::nhppp_t_intensity(
    lambda = l, lambda_maj = l_max, range_t = tp, rng_stream = NULL, only1 = T
  ),
  "(2e) R: Thinning (best args, one sample, no substream, oblique majoriser)" = nhppp::nhppp_t_intensity(
    lambda = l, lambda_maj = c(1, 2), range_t = tp, rng_stream = NULL, only1 = T
  ),
  "(2f) R: Thinning (worse args, all samples, no substream)" = nhppp::nhppp_t_intensity(lambda = l, lambda_maj = NULL, range_t = tp, only1 = FALSE),
  check = F, filter_gc = T, max_iterations = n_times
)
```

```{r}
# Plot of results
xpos <- comptimes$median
# xpos[1] <- xpos[1] - 0.00001
# xpos[c(7, 11)] <- xpos[c(7, 11)] - 0.00007
# xpos[8] <- xpos[8] - 0.00009
# xpos[10] <- xpos[10] - 0.00005
# xpos[c(9, 12:13, 15)] <- xpos[c(9, 12, 15)] + 0.0002
p3 <- autoplot(comptimes, type = c("ridge")) +
  labs(x = "", y = "") +
  ggtitle("") +
  theme(
    axis.text.x = element_text(size = 8),
    axis.text.y = element_text(size = 8)
  ) +
  scale_y_discrete(
    limits = rev,
    labels = function(x) str_wrap(x, width = 45)
  ) +
  annotate("text",
    x = xpos,
    y = c(length(comptimes$median):1),
    label = paste0("M=", gsub(" ", "", comptimes$median,
      fixed = TRUE
    )),
    size = 3, vjust = 1.5, hjust = 0.5
  ) +
  theme(
    axis.text.x = element_text(size = 8),
    axis.text.y = element_text(size = 10),
    plot.margin = margin(t = 0, r = 1, b = 0, l = 0, "cm")
  )
# ggsave("comptimes.pdf", plot=p3, device=cairo_pdf, height = 6, width=8)
p3
```

```{r}
# Computational time across packages
comptimes2 <- bench::mark(
  "(1a) IndTestPP: thinning (all)" = IndTestPP::simNHPc(lambda = l(seq(tp[1], tp[2], length.out = tp[2])), fixed.seed = NULL, algor = "Thinning")$posNH,
  "(1b) IndTestPP: inversion (all)" = IndTestPP::simNHPc(lambda = l(seq(tp[1], tp[2], length.out = tp[2])), fixed.seed = NULL, algor = "Inversion")$posNH,
  # NHPoisson returns indices of moments of events instead of actual times!
  # The length of t=375 returns 100 events on average
  "(2) NHPoisson: inversion (all)" = NHPoisson::simNHP.fun(lambda = l(seq(tp[1], tp[2], length.out = 375)))$posNH,
  "(3) poisson: thinning (all)" = poisson::nhpp.event.times(rate = l_max, num.events = L(tp[2]) - L(tp[1]), prob.func = function(t) {
    l(t) / l_max
  }, num.sims = 1, t0 = 0),
  "(4) PtProcess: thinning (all)" = simulate(
    object = PtProcess::mpp(
      data = NULL,
      gif = PtProcess::simple_gif, marks = list(NULL, NULL),
      # Function: \lambda(t) = a + bt^g
      params = c(a = 0, b = 2, g = 1),
      gmap = expression(params), mmap = NULL, TT = tp
    ),
    # Expected count of events \Lambda(t_max) - Lambda(t_min)
    max.rate = PtProcess::simple_gif(NULL, NULL, c(a = 0, b = 2, g = 1), tp)
  )$data[[1]],
  "(5a) reda: thinning (one)" = reda::simEvent(
    rho = l,
    origin = tp[1], endTime = tp[2],
    recurrent = F,
    method = "thinning"
  ),
  "(5b) reda: inversion (one)" = reda::simEvent(
    rho = l,
    origin = tp[1], endTime = tp[2],
    recurrent = F,
    method = "inversion"
  ),
  "(5c) reda: thinning (all)" = reda::simEvent(
    rho = l,
    origin = tp[1],
    endTime = tp[2],
    recurrent = T,
    method = "thinning"
  ),
  "(5d) reda: inversion (all)" = reda::simEvent(
    rho = l,
    origin = tp[1],
    endTime = tp[2],
    recurrent = T,
    method = "inversion"
  ),
  "(6) simEd: thinning (all)" = simEd::thinning(
    maxTime = tp[2],
    intensityFcn = l,
    majorizingFcn = NULL,
    majorizingFcnType = NULL,
    seed = NA,
    maxTrials = Inf,
    plot = F
  ),
  check = F, filter_gc = T, max_iterations = nsim, memory = F
)
```

```{r}
# Plot of results
xpos <- comptimes2$median
xpos[6] <- xpos[6] - 0.0005
xpos[7] <- xpos[7] + 0.0009
p4 <- autoplot(comptimes2, type = c("ridge")) +
  labs(x = "", y = "") +
  ggtitle("") +
  theme(
    axis.text.x = element_text(size = 8),
    axis.text.y = element_text(size = 8)
  ) +
  scale_y_discrete(
    limits = rev,
    labels = function(x) str_wrap(x, width = 40)
  ) +
  annotate("text",
    x = xpos,
    y = c(length(comptimes2$median):1),
    label = paste0("M=", gsub(" ", "", comptimes2$median,
      fixed = TRUE
    )),
    size = 3, vjust = 1.5, hjust = 0.5
  ) +
  theme(
    axis.text.x = element_text(size = 8),
    axis.text.y = element_text(size = 10),
    plot.margin = margin(t = 0, r = 1, b = 0, l = 0, "cm")
  )
# ggsave("comptimes2.pdf", plot=p4, device=cairo_pdf, height = 6, width=8)
p4
```
